<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Security | Oh The Huge Manatee]]></title>
  <link href="http://ohthehugemanatee.github.io/blog/categories/security/atom.xml" rel="self"/>
  <link href="http://ohthehugemanatee.github.io/"/>
  <updated>2016-11-01T08:20:11+01:00</updated>
  <id>http://ohthehugemanatee.github.io/</id>
  <author>
    <name><![CDATA[Campbell Vertesi (ohthehugemanatee)]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[SSL Certificates Demystified]]></title>
    <link href="http://ohthehugemanatee.github.io/blog/2013/06/20/ssl-certificates-demystified/"/>
    <updated>2013-06-20T00:00:00+02:00</updated>
    <id>http://ohthehugemanatee.github.io/blog/2013/06/20/ssl-certificates-demystified</id>
    <content type="html"><![CDATA[<p>It seems like any time anyone has to configure a server for SSL/TLS, they treat it like some kind of voodoo magic. And I guess it can seem that way: if you get your certificates in the right formats from your certificate authority, you basically just drop them into Apache/NGINX/whatever and let it go.&nbsp;</p>


<p>But what happens if those files&nbsp;<strong>aren't</strong> in the right format by default? And what if you actually want to understand what's going on with your server?</p>


<p>The truth is, encryption is not terribly difficult to understand. It's a little tricky to explain, and the maths in the nitty gritty of it can get really intense, but a high level understanding is important for anyone who works with SSL.&nbsp;</p>


<div>The basis for SSL is <a href="https://en.wikipedia.org/wiki/Public_key_cryptography">public-key</a> (asymmetrical) cryptography. The idea is that you create a pair of files that have matching abilities:</div>


<div>&nbsp;</div>


<div>*<strong> the "public" file can&nbsp;encrypt&nbsp;content</strong> so that the matching "private" file must be used to decrypt it. It can also <strong>verify the signature</strong> of the "private" file.</div>


<div>&nbsp;</div>


<div>* <strong>the "private" file can&nbsp;decrypt&nbsp;content</strong> that was encrypted by the matching "public" file. It can also <strong>leave a unique digital signature</strong> which can be verified by the matching "public" file.</div>


<div>&nbsp;</div>


<div>If you've ever used the <a href="https://en.wikipedia.org/wiki/Pretty_Good_Privacy">PGP encryption</a> system, that's basically how it works. You publish your public key in a publicly available directory, and keep your private key to yourself. That way, anyone can use your public key to encrypt content for you, and only you can decrypt it.</div>


<div>&nbsp;</div>


<div>In the context of web communications, this model is taken to the next level. We use the encryption aspect of it, but we also use those digital signatures. Here's the process.</div>


<div>&nbsp;</div>


<div>1) You create this "key pair" for yourself, and hide the private key away somewhere. Optionally the private key can be password-protected.</div>


<div>&nbsp;</div>


<div>2) You create a Certificate Request file (.CSR, usually). A Certificate Request is just a copy of your Public Key, with some identifying information about your organization. The most important piece of this information is the domain name this Request is for. You submit your CSR file to a Certificate Authority (abbreviated as "CA) company like <a href="https://www.rapidssl.com/">RapidSSL</a>, or any number of other Certificate Authorities out there.</div>


<div>&nbsp;</div>


<div>3) The CA verifies that you really own the domain name specified in the CSR. How they verify that is up to the CA. Maybe they email the owner of the domain, maybe they ask you to modify a DNS record, maybe they just take you on your word (hopefully not!). Whatever they do, when the CA is satisfied that you really do own that domain name, they send you back your Certificate Request file. This version of the file is signed by the CA's own private key, and that difference makes it the actual "SSL certificate" file. If you've been keeping score at home, the SSL certificate is simply your public key with your organizational information attached, and your CA's signature.&nbsp;</div>


<div>&nbsp;</div>


<div>Great, now people can send you encrypted messages, right? Well, sorta. How do people know they can trust the CA? Maybe that CA is just some yahoo who lives up the hall, or your great aunt Edna. How can I trust Edna to vouch for you?&nbsp;</div>


<div>&nbsp;</div>


<div>4) The solution is a "chain of authority" file. This is just your CA's public key (which the browser needs to check that signature on your SSL certificate, remember?). What makes this public key special is that it's ALSO signed by whoever granted your CA permission to be a CA, and by whoever granted THAT person the permission to be a CA, all the way back to one of the "root" certificate authorities, whose public keys are bundled with every browser. (there are about 30 of them). So the chain file from aunt Edna might contain signatures from her daughter Annie, Annie's company AnnieCA, and GeoTrust, who granted AnnieCA the authority to give out certificates. GeoTrust is a root CA, so your browser already knows they're trusted. &nbsp;</div>


<div>&nbsp;</div>


<div>So when a web browser connects to your server, your server sends out its SSL certificate file (ie the server's public key with Edna's "Seal of Authenticity" on it). The browser can then encrypt communications to send to your server, and your server can decrypt them with its private key. The browser can also check on Edna's Seal of Authenticity, because there's a chain of other CAs who have signed Edna's certificate, right back to one of the CAs that the browser already trusts.</div>


<div>&nbsp;</div>


<div>With a one-way secure channel open, and with confidence that your server really does own that public key, the browser sends its own public key to the server. This is really as far as you need to understand in order to set up your own webserver. Once the server and browser have exchanged public keys, the rest is automatic. For the sake of completion in this blog post, you should know that they don't continue using this asymmetric encryption for long. This secure channel is only used to set up a symmetric encryption method - that is, an encryption method where both sides have identical keys for encryption and decryption. &nbsp;The actual content is sent over this symmetrically encrypted connection, because symmetrical encryption is lighter on the CPU and the data overhead, and is actually harder to break.</div>


<div>&nbsp;</div>


<div>All we sysadmins really care about is the handshake, because that's where all those confusing key files go. So, TL;DR time. Here's what you need:</div>


<div>&nbsp;</div>


<div>1) Your server's private key file. Note that this can be encrypted with a password, but most web hosts would rather you not. If there's a password, someone has to enter it every time they restart the server!</div>


<div>2) The SSL Certificate, signed by your Certificate Authority.</div>


<div>3) Your Certificate Authority's chain of intermediate certificates.</div>


<div>&nbsp;</div>


<div>Most (Drupal-centric) web hosts will want all of these in PEM format. PEM is a text format, which means you can just copy and paste the key around. If your certificates are in another format, it's pretty easy to convert them to PEM with the openssl command, like this:</div>


<div>&nbsp;</div>


<div>
<pre class="brush: bash; auto-links: true; collapse: false; first-line: 1; html-script: false; smart-tabs: true; tab-size: 4; toolbar: true; codetag"> openssl &lt;old-cert-format&gt; -in old-cert-file -out new-cert-file.pem </pre>
</div>


<div>There are also a handful of options that can come in handy in specifying the output, but they really go beyond the scope of this blog post. The resource I always use is the <a href="https://twiki.cern.ch/twiki/bin/view/LinuxSupport/OpenSSLCheatsheet">CERN openSSL conversions cheat sheet</a>.&nbsp;</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What the Sony Hacks Tell Us About Their Sysadmins and Management]]></title>
    <link href="http://ohthehugemanatee.github.io/2011/05/what-sony-hacks-tell-us-about-their.html"/>
    <updated>2011-05-21T00:04:00+02:00</updated>
    <id>http://ohthehugemanatee.github.io/2011/05/what-sony-hacks-tell-us-about-their</id>
    <content type="html"><![CDATA[<p></p>

<div class="css-full-post-content js-full-post-content">
I wouldn't want to be a Sony sysadmin right now. <a href="http://www.f-secure.com/weblog/archives/00002160.html">F-Secure just blogged</a> about evidence of a <strong>fourth</strong> hack at the electronics giant.   This one is relatively harmless - a phishing scam being run from Sony servers - but this pattern of security problems should tell us a few things about Sony's SysAdmin staff:<br /><br />1) They're powerless in their own organization.<br />2) They don't get to set IT resource policy.<br />3) They're furious at their bosses right now.<br /><br />It's hard to generalize like this across an entire organization, but I'm willing to bet that in the affected departments this is the case.  Let's look at what we know about the hacks.  <br /><br />The first two involved stealing millions of users' credit card information, which was stored unencrypted and vulnerable during a DoS.  The hackers got in with a list of valid usernames and a dictionary attack.  I don't know any SysAdmin, no matter how junior, who would leave 100 million users' financial and personal information in an unencrypted format.  And on a server with access to this unencrypted financial information, who doesn't impose password standards?  I can understand a vulnerability during an exceptional circumstance - a really good SysAdmin team has contingency plans for this sort of thing, but at least it's an understandable problem - but dammit, that data was just left out in the open, with only a dictionary password to protect it!  The only thing the DoS did was cover for the dictionary attempts. What SysAdmin team <i>does</i> that?<br /><br />The next "attack" was really just a vulnerability in the password reset system.  If you knew a user's date of birth and email address, you could gain access to their account.  Again, who actually deploys a system that vulnerable on an enterprise level?  Again, this is an account that includes personal, financial data.  Not even an email confirmation, or personal security question in sight!  Remember this is a service that has a purpose built platform associated with it.  They could ask for the serial number off the back of your PS3, or send you a code to your Playstation Mobile.  They could set up an RSA-style key generator on your device, the way Google does with your mobile phone.  But no - your financial data was protected by information that you publish on Facebook.  What technical lead <i>does</i> that?<br /><br />And finally, this most recent attack is a more conventional hack on a (relatively) unimportant server. It's true that by now, Sony's many arms must be the target of every wannabe hacker with a cablemodem.  But seriously, don't these guys run updates?  Do they have a vulnerability scanner?  Nessus should give them a license as a charity case.<br /><br />I think the most damning part is the response to the hacks.  In a gesture to try and restore their reputation on security, Sony laid out their technical response:<br /><br /><ul><li>Added automated software monitoring and configuration management to help defend against new attacks</li><li>Enhanced levels of data protection and encryption</li><li>Enhanced ability to detect software intrusions within the network, unauthorized access and unusual activity patterns</li><li>Implementation of additional firewalls</li></ul><br />Wait a moment - are they telling us they DIDN'T have these things before?  SONY, of all companies, didn't have an intrusion detection system?  They didn't encrypt their data?  This megagiant of digital services had no internal firewalls, no security scans?  <br /><br /><h2>Don't blame the SysAdmins</h2><br />It seems like I'm going to come down on the SysAdmin team over there.  I'm not.  I know what it's like working in a business environment.  Most importantly, I know what it's like working in an environment where a <i>manager</i>, not a technical person, is calling the shots.  And to me, this reeks of it.  <br /><br />This smacks of a work environment where the technical people are told not to "waste" time on things like "updates", or to make things too complicated with "security".  A workplace where the SysAdmins have been saying for months that this stuff is important, but management has set other priorities.  <br /><br />It's frustrating being in that kind of environment on a day to day basis, but people get by.  There's plenty of new work to get to, plenty of issues that your boss thinks are more profitable than upkeep.  And the admins can't understand why the bosses don't want to spend time on these critical, invisible tasks.  It seems like such a simple principle, and there are a million metaphors for it: feeding the golden goose, getting your teeth checked, getting an oil change... whatever you want to call it, management is not interested.  And when the irresponsibility of management priorities finally comes home to roost, it's the SysAdmins who have to stay weekends and nights to fix it.  It's the SysAdmins who have to explain to everyone that data was unencrypted, that security patches weren't applied, that pentesting was never considered.  <br /><br />I've seen that happen at too many organizations to count, and I predict that a lot of Sony SysAdmins are secretly trying to find other jobs right now.  Looking for a new SysAdmin?  Pick up one of the guys at Sony!  They'd be happy for a working environment that lets them do their job.
</div>


<p></p>
]]></content>
  </entry>
  
</feed>
