<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Sysadmin | Oh, The Huge Manatee]]></title>
  <link href="http://ohthehugemanatee.org/blog/categories/sysadmin/atom.xml" rel="self"/>
  <link href="http://ohthehugemanatee.org/"/>
  <updated>2019-02-11T13:57:22+01:00</updated>
  <id>http://ohthehugemanatee.org/</id>
  <author>
    <name><![CDATA[Campbell Vertesi (ohthehugemanatee)]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Kubernetes for Stateful Applications: Scaling Macroservices]]></title>
    <link href="http://ohthehugemanatee.org/blog/2019/01/07/kubernetes-tricks-for-stateful-applications/"/>
    <updated>2019-01-07T11:10:21+01:00</updated>
    <id>http://ohthehugemanatee.org/blog/2019/01/07/kubernetes-tricks-for-stateful-applications</id>
    <content type="html"><![CDATA[<p>I recently got to proctor an <a href="https://openhack.microsoft.com/">Openhack</a> event on modern containerization. It ended up an excuse to dig deep on one of the corner cases that we all encounter, but no one likes to talk about.</p>

<p><a href="https://kubernetes.io/">Kubernetes</a> is one of the greatest <strong>orchestration</strong> and <strong>scaling</strong> tools ever built, designed for modern <strong>decoupled</strong>, <strong>stateless</strong> architectures. Kubernetes tutorials abound to show you these strong use cases. <strong>But in the real world where you don&rsquo;t get to build &ldquo;green field&rdquo; every time, there are a lot of applications that don&rsquo;t fit that model</strong>.</p>

<p>Lots of people out there are still writing tightly-coupled monoliths, in many cases for good reason. In some use cases microservices style scalability isn&rsquo;t even useful &ndash; you actually <em>prefer</em> stateful applications with tight coupling. For example a game server, where you don&rsquo;t want to scale player capacity per-game, you want to add more games (server instances).</p>

<p>So today I&rsquo;m writing about <strong>stateful, non-scalable applications in kubernetes.</strong></p>

<p>There are a few different approaches to coupling appliciation components:</p>

<h2>Multi-container pods</h2>

<p>Level 0 is to simply specify multiple components (containers) in your deployment.</p>

<p>``` yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:</p>

<pre><code>app: nginx
</code></pre>

<p>spec:
  replicas: 3
  selector:</p>

<pre><code>matchLabels:
  app: nginx
</code></pre>

<p>  template:</p>

<pre><code>metadata:
  labels:
    app: nginx
spec:
  containers:
  - name: php
    image: php:fpm
    ports:
    - containerPort: 9000
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
</code></pre>

<p>```
This specifies 3 copies of the same application, with the same two containers in each replica. This is a coupled application, but it&rsquo;s still stateless. Let&rsquo;s add a volume &ndash; that&rsquo;s where we get into trouble.</p>

<p>The problem: If you add a Volume the normal way (<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">persistentVolumeClaim</a>), each of your replicas will try and connect to the same volume. It&rsquo;ll act like a network shared drive. Maybe that&rsquo;s OK for your application, but not if it&rsquo;s our super-stateful example! And depending on your volume class, the volume may reject multiple connections like (<a href="https://docs.microsoft.com/en-us/azure/aks/azure-disks-dynamic-pv">Azure Disk</a> does, for example).</p>

<p>So how do we get around this limitation? I want a separate volume for each instance of the application.</p>

<p>Kubernetes supports a different object type for this use case, called a <a href="https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/">StatefulSet</a>. This is exactly what it sounds like: a set of objects that define a stateful application. It&rsquo;s a template for creating multiple copies of <em>all resources</em> defined therein.</p>

<p>A statefulset will create replicas similar to a deployment, but it will set up separate Volumes and VolumeClaims for each one. The replicas will be identical except for an index number at the end of the labels. The first one might be called <code>nginx-deployment-0</code>, the second: <code>nginx-deployment-1</code>, and so on.  The result is a set of tightly coupled components, which can be individually addressed, and scaled using normal Kubernetes tools.</p>

<p>``` yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nginx
  labels:</p>

<pre><code>app: nginx
</code></pre>

<p>spec:
  replicas: 3
  selector:</p>

<pre><code>matchLabels:
  app: nginx
</code></pre>

<p>  serviceName: &ldquo;nginx&rdquo;
  template:</p>

<pre><code>metadata:
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
  - name: php
    image: php:fpm
    volumeMounts:
    - mountPath: "/var/www/html"
      name: data
</code></pre>

<p>  volumeClaimTemplates:</p>

<pre><code>- metadata:
    name: data
  spec:
    storageClassName: default
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 5Gi
</code></pre>

<hr />

<p>apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:</p>

<pre><code>app: nginx
</code></pre>

<p>spec:
  ports:</p>

<pre><code>- port: 80
  name: http
</code></pre>

<p>  clusterIP: None
  selector:</p>

<pre><code>app: nginx
</code></pre>

<p>```
There are a few details to notice here.</p>

<p>Yes, we&rsquo;ve replaced <em>Deployment</em> with <em>StatefulSet</em>. You get a shiny gold star if you noticed that one.</p>

<p>The interesting part is the <em>VolumeClaimTemplates</em> section, below the containers definition. This keyword only exists inside a StatefulSet, and it&rsquo;s just what it sounds like: a template for creating Persistent Volume Claims.</p>

<p>If you apply this config, you&rsquo;ll see three PVs created, with three PVCs, attached to three Pods. You can apply HPA rules to scale these up and down just like you would with deployments.</p>

<p>There&rsquo;s also that weird Service at the bottom. A naked service with no clusterIP? What&rsquo;s the point? The point is as a helper for Kubernetes' internal DNS. All of those nice StatefulSet pods will come under a neat subdomain, eg nginx-0.nginx, nginx-1.nginx, etc. Additionally you can connect to active members of the StatefulSet by using that nginx domain component. A dns lookup on it will show all the IPs of the active members in the CNAME record.</p>

<p>&ldquo;But what about external access?&rdquo; I hear you cry. Yes, we&rsquo;ve built a great stateful application that can scale instances, but it&rsquo;s only internally addressable! Good luck hosting those games&hellip;</p>

<h2>External access and metacontroller</h2>

<p>Normally you would put a LoadBalancer service in front of your application. But a Kubernetes load balancer will grab all of these StatefulSet members &ndash; so you can&rsquo;t address them externally one-by-one. What you <em>really</em> want to do, is create an external IP address for each statefulset member.</p>

<p>One solution is to use a reverse proxy like nginx or HAProxy, configured to differentiate based on hostnames. But this is a blog post about Kubernetes, so we&rsquo;re going to do this the Kubernetes way!</p>

<p>Kubernetes is very extensible. If Pods, Services, etc don&rsquo;t make sense for your application or domain, you can define custom object types and behaviors, through <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources and controllers</a>. That&rsquo;s pretty edge case, but as we&rsquo;ve seen, some kubernetes edge cases are mainstream cases in the real world.</p>

<p>In our super-stateful application, we don&rsquo;t need a custom resource type. But we do want to attach <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#custom-controllers">custom behaviors</a> to our StatefulSet: every time we start up a pod we should create a LoadBalancer for it. We should be nice and tear them down when the pods are scaled down, of course.</p>

<p>We&rsquo;ll use the <a href="https://github.com/GoogleCloudPlatform/metacontroller">Metacontroller</a> add-on to make our lives easier. Metacontroller makes it &ldquo;easy&rdquo; to add custom behaviors. Just write a short script, stick it into a ConfigMap or FaaS, and let Metacontroller work its magic!</p>

<p>Metacontroller project comes with several well documented examples, including one that&rsquo;s very close to our requirement: <a href="https://github.com/GoogleCloudPlatform/metacontroller/tree/master/examples/service-per-pod">service-per-pod</a>.</p>

<p>Step 1 is to install Metacontroller, of course:</p>

<p>``` bash</p>

<h1>Create &lsquo;metacontroller&rsquo; namespace, service account, and role/binding.</h1>

<p>kubectl apply -f <a href="https://raw.githubusercontent.com/GoogleCloudPlatform/metacontroller/master/manifests/metacontroller-rbac.yaml">https://raw.githubusercontent.com/GoogleCloudPlatform/metacontroller/master/manifests/metacontroller-rbac.yaml</a></p>

<h1>Create CRDs for Metacontroller APIs, and the Metacontroller StatefulSet.</h1>

<p>kubectl apply -f <a href="https://raw.githubusercontent.com/GoogleCloudPlatform/metacontroller/master/manifests/metacontroller.yaml">https://raw.githubusercontent.com/GoogleCloudPlatform/metacontroller/master/manifests/metacontroller.yaml</a>
```</p>

<p>Then we&rsquo;ll add some new metadata to our existing StatefulSet. The metacontroller script will use these values to configure the load balancers.</p>

<p>``` yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  annotations:</p>

<pre><code>service-per-pod-label: "pod-name"
service-per-pod-ports: "80:80"
</code></pre>

<p>&hellip;
```</p>

<p>We also need to tell Kubernetes to decorate each StatefulSet with a pod-name label. We do this in the StatefulSet&rsquo;s pod template.</p>

<p>``` yaml
&hellip;
spec:
  template:</p>

<pre><code>metadata:
  annotations:
    pod-name-label: "pod-name"
</code></pre>

<p>&hellip;
```</p>

<p>Note: this only works in k8s 1.9+ &ndash; if you&rsquo;re stuck with a lower version, you can script this action with Metacontroller, too. :).</p>

<p>Now you&rsquo;re going to need two hooks. Put them in a directory together so they&rsquo;re easy to apply at once. These ones are written in jsonnet, but you could write this in whatever language you like.</p>

<p>The first hook actually creates the LoadBalancer for each Pod.</p>

<p>``` c#
function(request) {
  local statefulset = request.object,
  local labelKey = statefulset.metadata.annotations[&ldquo;service-per-pod-label&rdquo;],
  local ports = statefulset.metadata.annotations[&ldquo;service-per-pod-ports&rdquo;],</p>

<p>  // Create a service for each Pod, with a selector on the given label key.
  attachments: [</p>

<pre><code>{
  apiVersion: "v1",
  kind: "Service",
  metadata: {
    name: statefulset.metadata.name + "-" + index,
    labels: {app: "service-per-pod"}
  },
  spec: {
    type: "LoadBalancer",
    selector: {
      [labelKey]: statefulset.metadata.name + "-" + index
    },
    ports: [
      {
        local parts = std.split(portnums, ":"),
        port: std.parseInt(parts[0]),
        targetPort: std.parseInt(parts[1]),
      }
      for portnums in std.split(ports, ",")
    ]
  }
}
for index in std.range(0, statefulset.spec.replicas - 1)
</code></pre>

<p>  ]
}
```
The other hook is the &ldquo;finalizer&rdquo; &ndash; it responds to changes or deletions in pods by tearing down the corresponding LoadBalancers.</p>

<p><code>c#
function(request) {
  // If the StatefulSet is updated to no longer match our decorator selector,
  // or if the StatefulSet is deleted, clean up any attachments we made.
  attachments: [],
  // Mark as finalized once we observe all Services are gone.
  finalized: std.length(request.attachments['Service.v1']) == 0
}
</code></p>

<p>Add those into a subdirectory, and put them into a configmap together. Metacontroller will run them from there.</p>

<p><code>bash
kubectl create configmap service-per-pod-hooks -n metacontroller --from-file=hooks
</code></p>

<p>Now apply the actual decorator controller which will run those functions. Note that you have to identify your hook jsonnet files by (file) name! Get the name wrong, and the finalizer will hang forever, <a href="https://github.com/kubernetes/kubernetes/issues/72598">preventing you from deleting your statefulset</a>. In my case, the files were called <code>create-lb-per-pod.jsonnet</code> and <code>finalizer.json</code>.</p>

<p>``` yaml
apiVersion: metacontroller.k8s.io/v1alpha1
kind: DecoratorController
metadata:
  name: service-per-pod
spec:
  resources:
  &ndash; apiVersion: apps/v1beta1</p>

<pre><code>resource: statefulsets
annotationSelector:
  matchExpressions:
  - {key: service-per-pod-label, operator: Exists}
  - {key: service-per-pod-ports, operator: Exists}
</code></pre>

<p>  attachments:
  &ndash; apiVersion: v1</p>

<pre><code>resource: services
</code></pre>

<p>  hooks:</p>

<pre><code>sync:
  webhook:
    url: http://service-per-pod.metacontroller/create-lb-per-pod
finalize:
  webhook:
    url: http://service-per-pod.metacontroller/finalizer
</code></pre>

<hr />

<p>apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: service-per-pod
  namespace: metacontroller
spec:
  replicas: 1
  selector:</p>

<pre><code>matchLabels:
  app: service-per-pod
</code></pre>

<p>  template:</p>

<pre><code>metadata:
  labels:
    app: service-per-pod
spec:
  containers:
  - name: hooks
    image: metacontroller/jsonnetd:0.1
    imagePullPolicy: Always
    workingDir: /hooks
    volumeMounts:
    - name: hooks
      mountPath: /hooks
  volumes:
  - name: hooks
    configMap:
      name: service-per-pod-hooks
</code></pre>

<hr />

<p>apiVersion: v1
kind: Service
metadata:
  name: service-per-pod
  namespace: metacontroller
spec:
  selector:</p>

<pre><code>app: service-per-pod
</code></pre>

<p>  ports:
  &ndash; port: 80</p>

<pre><code>targetPort: 8080
</code></pre>

<p>```</p>

<p>That&rsquo;s it! Now you can scale complete replicas of a very-stateful application with a simple <code>kubectl scale sts nginx --replicas=900</code>.</p>

<p>Enjoy bragging to your friends about your &ldquo;macroservices architecture&rdquo;, pushing the limits of Kubernetes to run and replicate a stateful monolith!</p>

<p><em>Everyone hates writing YAML. Check out the <a href="https://github.com/ohthehugemanatee/kubernetes-stateful-example">sample code for this post on Github</a></em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Optimizing Data Transfer Speeds]]></title>
    <link href="http://ohthehugemanatee.org/blog/2018/12/27/optimizing-data-transfer-speeds/"/>
    <updated>2018-12-27T10:30:23+01:00</updated>
    <id>http://ohthehugemanatee.org/blog/2018/12/27/optimizing-data-transfer-speeds</id>
    <content type="html"><![CDATA[<p>One of my holiday projects was to set up my home &ldquo;data warehouse.&rdquo; Ever since <a href="https://www.dropboxforum.com/t5/Syncing-and-uploads/Linux-Dropbox-client-warn-me-that-it-ll-stop-syncing-in-Nov-why/m-p/290065/highlight/true#M42255">Dropbox killed modern Linux filesystem support</a> I&rsquo;ve been using (and loving) <a href="https://nextcloud.com/">Nextcloud</a> from my home. It backs up to an encrypted <a href="https://www.duplicati.com/">Duplicati</a> store on <a href="https://azure.microsoft.com/en-us/services/storage/blobs/">Azure blob store</a>, so that&rsquo;s offsite backups taken care of. But it was time to knit all my various drives together into a single RAID data warehouse. The only problem: how to transfer my 2 terabytes (rounded to make the math in the post easier) of data, without nasty downtime during the holidays?</p>

<p>A local network transfer is the fastest, with the least downtime. I have a switched gigabit network in my house, and all my servers are hard wired. That&rsquo;s about 125 megabytes per second; a theoretical 5 hours to transfer everything. Not bad! Start up an rsync and I&rsquo;m all done! So I kicked it off and went to bed:</p>

<p><code>bash
$ ssh nextcloud.vert
$ rsync -axz /media/usbdrive/ warehouse:/mnt/storage/ --log-file=transfer-to-warehouse.log &amp;
</code></p>

<p>I woke up in the morning with the excitement of a kid on Christmas. Everything should be done, right?</p>

<p><code>bash
$ ssh warehouse df -h |grep md0
/dev/md0        2.7T  501G  2.1T  20% /mnt/storage
$
</code>
Wait, what? How had it only transferred 500 gigabytes overnight? Including time for <em>Doctor Who</em> and breakfast, that was only 1 Megabit per second! I knew it was time to play everyone&rsquo;s favorite game: <em>&ldquo;where&rsquo;s the bottleneck?</em></p>

<p>I guess it could be rsync scanning all those small files. If that&rsquo;s the case, we&rsquo;ll see high CPU usage, and even higher load numbers (as processes are I/O blocked):</p>

<p>``` bash
$ ssh nextcloud
$ top</p>

<p>top &ndash; 08:22:27 up 10:26,  1 user,  load average: 1.20, 1.34, 1.33
Tasks: 170 total,   2 running, 106 sleeping,   0 stopped,   0 zombie
%Cpu(s): 28.0 us,  2.1 sy,  0.0 ni, 69.5 id,  0.1 wa,  0.0 hi,  0.3 si,  0.0 st
KiB Mem : 16330372 total,   180568 free,   657104 used, 15492700 buff/cache
KiB Swap:  4194300 total,  4162556 free,    31744 used. 15300068 avail Mem</p>

<p>  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                <br/>
 8755 ohthehu+  20   0  130572  58456   2672 R  99.0  0.4 513:14.75 rsync                                                                                  <br/>
 8756 ohthehu+  20   0   49596   6648   5152 S  16.9  0.0  92:12.29 ssh
&hellip;
```</p>

<p>OK, let&rsquo;s kill the transfer and start again using a single large, piped tarball. No more small file scans!</p>

<p><code>bash
$ ssh nextcloud
$ cd /media/bigdrive &amp;&amp; tar cf - . | ssh warehouse "cd /mnt/storage &amp;&amp; tar xpvf -"
</code></p>

<p>That helps, but we&rsquo;re still compressing lots of data unnecessarily (most of my data is already compressed), and encrypting it, too. We can improve it with a lightweight ssh cipher and disabled compression:</p>

<p><code>bash
$ ssh nextcloud
$ cd /media/bigdrive &amp;&amp; tar cf - . | ssh -o Compression=no -c chacha20-poly1305@openssh.com warehouse "cd /mnt/storage &amp;&amp; tar xpf -"
</code></p>

<p>That chacha20-poly1305 is a very fast cipher indeed &ndash; faster than the old arcfour cipher we used to use in this case. But SSH still puts extra work on the CPU. So let&rsquo;s remove it completely from the equation and just use netcat.</p>

<p>``` bash
$ ssh nextcloud cd /media/bigdrive &amp;&amp; tar cf &ndash; . | pv | nc -l -q 5 -p 9999</p>

<h1>in a separate terminal</h1>

<p>$ ssh warehouse cd /mnt/storage &amp;&amp; nc nextcloud 9999 | pv | tar -xf &ndash;
```</p>

<p>Transfer speeds now average about 61 megabytes per second. That&rsquo;s fast enough to kick in the law of diminishing returns on my optimization effort: this will take about 8 hours to transfer if I keep it running. I had to pause work for an hour; now if I spend another hour on this, it has to shave more than 25% off my transfer time to finish any earlier tonight. I&rsquo;m not confident I can beat those numbers.</p>

<p>Still &ndash; What happened to my 125 theoretical megabytes per second? Here are the culprits I suspect &ndash; and can&rsquo;t really do anything about:</p>

<ul>
<li><p><strong>Slow disk</strong>: We are writing to a software RAID5 array of old drives. In my head I was using the channel width of SATA-II for my calculations. In reality, and especially on spinning metal, write speeds are much slower. I looked up my component disks on <a href="userbenchmark.com">userbenchmark.com</a>, and the slowest member has an average sustained sequential write speed of 69 MB/s. This is very likely my first bottleneck. At most I can only use half of my available bandwidth.</p></li>
<li><p><strong>TCP</strong>: After replacing all my drives with SSDs, TCP is the next culprit I would go after. The protocol technically only has about 6% of overhead, but it also dynamically seeks the maximum send rate through <a href="https://en.wikipedia.org/wiki/TCP_congestion_control">TCP Congestion Control</a>. It keeps trying to send &ldquo;just a little faster&rdquo;, until the number of unacknowledged packets exceeds a threshold. Then it backs off to 50%, and goes back to &ldquo;just a little faster&rdquo; mode. This loop means your practical speed with a TCP stream is about 75% of the pipe&rsquo;s theoretical maximum. Think of it like John Cleese offering just one more &ldquo;wafer thin&rdquo; packet. I considered using UDP to avoid this, but I actually <em>want</em> the error-checking in TCP. Probably the best solution is <a href="https://github.com/LabAdvComp/UDR">something esoteric like UDR</a>.</p></li>
<li><p><strong>Slow CPU</strong>: This is the last bottleneck here. <em>Warehouse</em> is an old Intel Core2 Duo I had lying around the house. Untar and netcat aren&rsquo;t exactly CPU hungry beasts, but at some point there IS a maximum. If you believe the FreeNAS folks, a fileserver needs an i5 and 8 gigs of RAM for basic functionality. I haven&rsquo;t found that to be the case, but then I&rsquo;m not using RAID-Z, either.</p></li>
</ul>


<p>I&rsquo;m happy with the outcome here. I have another drive to copy later, with another terabyte. I&rsquo;m considering removing that slowest drive from my RAID array, since the next-slowest one is almost 50% faster. Then I can copy to the array while it&rsquo;s in degraded mode, and re-add the slowpoke afterwards. We&rsquo;ll see.</p>

<p>Happy holidays!</p>

<h3>Appendix: easy performance testing</h3>

<p>If you&rsquo;re working on a similar problem for yourself, you might find these performance testing commands helpful. The idea is to tease apart each component of the transfer. There are better, more detailed, dedicated tools for each of these, but in a game of &ldquo;find the bottleneck&rdquo; you really only need quick and dirty validation. Fun fact: the command <em>dd</em> is actually short for <strong>D</strong>own and <strong>D</strong>irty. Well it should be, at any rate.</p>

<p><strong>Read speed (on the source</strong> is easy: hand an arbitrary large file to dd, and write down the numbers it gives.</p>

<p><code>bash
$ dd if=large-file.tar.bz2 of=/dev/null bs=1M
1021317200 bytes (1 GB) copied, 3.9888 s, 256 MB/s
</code></p>

<p><strong>Network speed</strong> can be tested by netcatting a gigabyte of zeros from one machine to the other.</p>

<p>``` bash</p>

<h1>On the receiving machine, open a port to /dev/null</h1>

<p>$ nc -vvlnp 12345 >/dev/null</p>

<h1>On the sending machine, send a gig of zeroes to that port</h1>

<p>$ dd if=/dev/zero bs=1M count=1K | nc -vvn 192.168.1.50 12345
Connection to 192.168.1.50 12345 port [tcp/*] succeeded!
1024+0 records in
1024+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 11.7811 s, 91.1 MB/s</p>

<h1>Remember, 8 bits to a byte!</h1>

<p>$ echo &ldquo;$(bc -l &lt;&lt;&lt; 91*8) Megabits&rdquo;
728 Megabits
```</p>

<p><strong>Write speed on the destination</strong> can be tested with dd, too:</p>

<p><code>bash
$ dd if=/dev/zero bs=1M count=1024 of=/mnt/storage/test.img
1024+0 records in
1024+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 55.3836 s, 19.4 MB/s
</code></p>

<p>(note: these tests were run while the copy was happening on warehouse. Your numbers should be better than this!)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What Crell Doesn't Want You to Know: How to Automate Letsencrypt on platform.sh]]></title>
    <link href="http://ohthehugemanatee.org/blog/2017/02/21/what-crell-doesnt-want-you-to-know-how-to-automate-letsencrypt-on-platform-dot-sh/"/>
    <updated>2017-02-21T22:33:08+01:00</updated>
    <id>http://ohthehugemanatee.org/blog/2017/02/21/what-crell-doesnt-want-you-to-know-how-to-automate-letsencrypt-on-platform-dot-sh</id>
    <content type="html"><![CDATA[<p>If you believe the <a href="https://docs.platform.sh/development/going-live.html#prerequisites">docs</a> and the <a href="https://twitter.com/damz/status/672559665377501184">twitters</a>, there is no way to automate <a href="https://letsencrypt.org/">letsencrypt</a> certificates updates on <a href="https://platform.sh/">platform.sh</a>. You have to create the certificates manually, upload them manually, and maintain them manually.</p>

<p>But as readers of this blog know, the docs are only the start of the story. I&rsquo;ve really enjoyed working with platform.sh with one of my private clients, and I couldn&rsquo;t believe that with all the flexibility &ndash; all the POWER &ndash; letsencrypt was really out of reach. I found a few attempts to script it, and one really great <a href="https://gitlab.com/snippets/27467">snippet on gitlab</a>. But no one had ever really synthesized this stuff into an easy howto. So here we go.</p>

<h3>1) Add some writeable directories where platform.sh CLI and letsencrypt need them.</h3>

<p>Normally when Platform deploys your application, it puts it all in a read-only filesystem. We&rsquo;re going to mount some special directories read-write so all the letsencrypt/platform magic can work.</p>

<p>Edit your application&rsquo;s <code>.platform.app.yaml</code> file, and find the <code>mounts:</code> section. At the bottom, add these three lines. Make sure to match the indents with everything else under the <code>mounts:</code> section!</p>

<p>```</p>

<pre><code>"/web/.well-known": "shared:files/.well-known"
"/keys": "shared:files/keys"
"/.platformsh": "shared:files/.platformsh"
</code></pre>

<p>```</p>

<p>Let&rsquo;s walk through each of these:</p>

<ul>
<li>/web/.well-known: In order to confirm that you actually control example.com, letsencrypt drops a file somewhere on your website, and then tries to fetch it. This directory is where it&rsquo;s going to do the drop and fetch. My webroot is <code>web</code>, you should change this to match your own environment. You might use <code>public</code> or <code>www</code> or something.</li>
<li>/keys: You have to store your keyfiles SOMEWHERE. This is that place.</li>
<li>/.platformsh: Your master environment needs a bit of configuration to be able to login to platform and update the certs on your account. This is where that will go.</li>
</ul>


<h3>2) Expose the .well-known directory to the Internet</h3>

<p>I mentioned above that letsencrypt test your control over a domain by creating a file which it tries to fetch over the Internet. We already created the writeable directory where the scripts can drop the file, but platform.sh (wisely) defaults to hide your directories from the Internet. We&rsquo;re going to add some configuration to the &ldquo;web&rdquo; app section to expose this .well-known directory. Find the <code>web:</code> section of your <code>.platform.app.yaml</code> file, and the <code>locations:</code> section under that. At the bottom of that section, add this:</p>

<p>```</p>

<pre><code>  '/.well-known':
        # Allow access to all files in the public files directory.
        allow: true
        expires: 5m
        passthru: false
        root: 'web/.well-known'
        # Do not execute PHP scripts.
        scripts: false
</code></pre>

<p>```</p>

<p>Make sure you match the indents of the other location entries! In my (default) <code>.platform.app.yaml</code> file, I have 8 spaces before that <code>'/.well-known':</code> line. Also note that the <code>root:</code> parameter there also uses my webroot directory, so adjust that to fit your environment.</p>

<h3>3) Download the binaries you need during the application &ldquo;build&rdquo; phase</h3>

<p>In order to do this, we&rsquo;re going to need to have the platform.sh CLI tool, and a let&rsquo;s encrypt CLI tool called lego. We&rsquo;ll download them during the &ldquo;build&rdquo; phase of your application. Still in the <code>platform.app.yaml</code> file, find the <code>hooks:</code> section, and the <code>build:</code> section under that. Add these steps to the bottom of the build:</p>

<p>```</p>

<pre><code>  cd ~
  curl -sL https://github.com/xenolf/lego/releases/download/v0.3.1/lego_linux_amd64.tar.xz | tar -C .global/bin -xJ --strip-components=1 lego/lego
  curl -sfSL -o .global/bin/platform.phar https://github.com/platformsh/platformsh-cli/releases/download/v3.12.1/platform.phar
</code></pre>

<p>```</p>

<p>We&rsquo;re just downloading reasonably recent releases of our two tools. If anyone has a better way to get the latest release of either tool, please let me know. Otherwise we&rsquo;re stuck keeping this up to date manually.</p>

<h3>4) Configure the platform.sh CLI</h3>

<p>In order to configure the platform.sh CLI on your server, we have to deploy the changes from steps 1-3. Go ahead and do that now. I&rsquo;ll wait.</p>

<p>Now connect to your platform environment via SSH (<code>platform ssh -e master</code> for most of us). First we&rsquo;ll add a config file for platform. Edit a file in <code>.platformsh/config.yaml</code> with the editor of choice. You don&rsquo;t have to use vi, but it will win you some points with me. Here are the contents for that file:</p>

<p>```
updates:</p>

<pre><code>check: false
</code></pre>

<p>api:</p>

<pre><code>token_file: token
</code></pre>

<p>```</p>

<p>Pretty straightforward: this tells platform not to bother updating the CLI tool automatically (it can&rsquo;t &ndash; read-only filesystem, remember?). It then tells it to login using an API token, which it can find in the file <code>.platformsh/token</code>. Let&rsquo;s create that file next.</p>

<p>Log into the platform.sh web UI (you can launch it with <code>platform web</code> if you&rsquo;re feeling sassy), and navigate to your account settings > api tokens. That&rsquo;s at <code>https://accounts.platform.sh/user/12345/api-tokens</code> (with your own user ID of course). Add an API token, and copy its value into <code>.platformsh/token</code> on the environment we&rsquo;re working on. The token should be the only contents of that file.</p>

<p>Now let&rsquo;s test it by running <code>php /app/.global/bin/platform.phar auth:info</code>. If you see your account information, congratulations! You have a working platform.sh CLI installed.</p>

<h3>5) Request your first certificate by hand</h3>

<p>Still SSH'ed into that environment, let&rsquo;s see if everything works.</p>

<p><code>
lego --email="support@example.com" --domains="www.example.com" --webroot=/app/public/ --path=/app/keys/ -a run
csplit -f /app/keys/certificates/www.example.com.crt- /app/keys/certificates/www.example.com.crt '/-----BEGIN CERTIFICATE-----/' '{1}' -z -s
php /app/.global/bin/platform.phar domain:update -p $PLATFORM_PROJECT --no-wait --yes --cert /app/keys/certificates/www.example.com.crt-00 --chain /app/keys/certificates/www.example.com.crt-01 --key /app/keys/certificates/www.example.com.key example.com
</code></p>

<p>This is three commands: register the cert with letsencrypt, then split the resulting file into it&rsquo;s components, then register those components with platform.sh. If you didn&rsquo;t get any errors, go ahead and test your site &ndash; it&rsquo;s got a certificate! (yay)</p>

<h3>6) Set up automatic renewals on cron</h3>

<p>Back to <code>.platform.app.yaml</code>, look for the <code>crons:</code> section. If you&rsquo;re running drupal, you probably have a drupal cronjob in there already. Add this one at the bottom, matching indents as always.</p>

<p>```</p>

<pre><code>letsencrypt:
    spec: '0 0 1 * *'
    cmd: '/bin/sh /app/scripts/letsencrypt.sh'
</code></pre>

<p>```</p>

<p>Now let&rsquo;s create the script. Add the file <code>scripts/letsencrypt.sh</code> to your repo, with this content:</p>

<p>``` bash</p>

<h1>!/usr/bin/env bash</h1>

<h1>Checks and updates the letsencrypt HTTPS cert.</h1>

<p>set -e</p>

<p>if [ &ldquo;$PLATFORM_ENVIRONMENT&rdquo; = &ldquo;master-7rqtwti&rdquo; ]
  then</p>

<pre><code># Renew the certificate
lego --email="example@example.org" --domains="example.org" --webroot=/app/web/ --path=/app/keys/ -a renew
# Split the certificate from any intermediate chain
csplit -f /app/keys/certificates/example.org.crt- /app/keys/certificates/example.org.crt '/-----BEGIN CERTIFICATE-----/' '{1}' -z -s
# Update the certificates on the domain
php /app/.global/bin/platform.phar domain:update -p $PLATFORM_PROJECT --no-wait --yes --cert /app/keys/certificates/example.org.crt-00 --chain /app/keys/certificates/example.org.crt-01 --key /app/keys/certificates/example.org.key example.org
</code></pre>

<p>fi
```</p>

<p>Obviously you should replace all those <code>example.org</code>s and email addresses with your own domain. Make the file executable with <code>chmod u+x scripts/letsencrypt.sh</code>, commit it, and push it up to your platform.sh environment.</p>

<h3>7) Send a bragging email to Crell</h3>

<p>Technically this isn&rsquo;t supposed to be possible, but YOU DID IT! Make sure to rub it in.</p>

<p><img class="center" src="/images/larry-garfield.jpg" title="&ldquo;Larry is waiting to hear from you. (photo credit Jesus Manuel Olivas)&rdquo;" ></p>

<p>Good luck!</p>

<p>PS &ndash; I&rsquo;m just gonna link one more time to the guy whose snippet made this all possible: <a href="https://www.drupal.org/u/hanoii">Ariel Barreiro</a> did the hardest part of this. I&rsquo;m grateful that he made his notes public!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Drush Self Aliases]]></title>
    <link href="http://ohthehugemanatee.org/blog/2014/01/10/drush-self-aliases/"/>
    <updated>2014-01-10T09:22:01+01:00</updated>
    <id>http://ohthehugemanatee.org/blog/2014/01/10/drush-self-aliases</id>
    <content type="html"><![CDATA[<p>I ran into an interesting problem with the drush <em>@self</em> alias today. I wanted to pull a fresh copy of the DB down from a client&rsquo;s live site to my local development copy. Should be as easy as <em>drush sql-sync @clientsite.live @self</em>, right? I&rsquo;ve done this a thousand times before.</p>

<p>And I&rsquo;ve also ignored the warning message every time before, but today I thought I&rsquo;d check it out:</p>

<blockquote><p>WARNING:  Using temporary files to store and transfer sql-dump.  It is recommended that you specify &mdash;source-dump and &mdash;target-dump options on the command line, or set &lsquo;%dump&rsquo; or &lsquo;%dump-dir&rsquo; in the path-aliases section of your site alias records. This facilitates fast file transfer via rsync.</p></blockquote>

<p>There are actually two possible solutions to this warning (that I can think of), and they illustrate some of the useful &ldquo;power user&rdquo; features of Drush that any frequent user should be aware of.</p>

<p>The warning is there because drush would <em>prefer</em> to rsync the DB dump from site1 to site2, rather than a one time copy. Rsync has lots of speed improvements, not the least being diff transfer. When transferring an updated copy of a file which already exists at the destination, rsync will only send over the changes rather than the whole file. This is pretty useful if you&rsquo;re dealing with a large, text based file like an SQL dump &ndash; especially one that you&rsquo;ll be transferring often. In order to use this efficient processing though, Drush needs to know a safe path where it can store the DB dump in each location.</p>

<p>First we&rsquo;ll add the <em>%dump-dir%</em> attribute to our alias for clientsite:</p>

<p>``` php ~/.drush/clientsite.aliases.drush.php
&lt;?php
// Site clientsite, environment live
$aliases[&lsquo;live&rsquo;] = array(
  &lsquo;parent&rsquo; => &lsquo;@parent&rsquo;,
  &lsquo;site&rsquo; => &lsquo;clientsite&rsquo;,
  &lsquo;env&rsquo; => &lsquo;live&rsquo;,
  &lsquo;root&rsquo; => &lsquo;/var/www/example.com/public_html&rsquo;,
  &lsquo;remote-host&rsquo; => &lsquo;example.com&rsquo;,
  &lsquo;remote-user&rsquo; => &lsquo;cvertesi&rsquo;,
  &lsquo;path-aliases&rsquo; => array(</p>

<pre><code>'%dump-dir' =&gt; '/home/cvertesi/.drush/db_dumps',
</code></pre>

<p>  ),
);
```</p>

<p>Notice that <em>%dump-dir</em> actually goes in a special sub-array for <em>path-aliases</em>. This is very likely the only time you&rsquo;ll need to use that section, since most everything else in there is auto-detected. This is the directory on the remote side where drush will store the dump.</p>

<p>Our options come in with the <em>@self</em> alias. In a local dev environment, the most common way to handle this is in your <em>drushrc.php</em> file:</p>

<p><code>php ~/.drush/drushrc.php
$options['dump-dir'] = '~/.drush/db_dumps';
</code></p>

<p>But this won&rsquo;t work for all cases. You can also take advantage of Drush&rsquo;s alias handling by creating a site alias with the settings you want, and letting Drush merge those settings into <em>@self</em>. When Drush builds its' cache of path aliases, it uses the site path as the cache key (for local sites only). That means that if you have a local alias with the same path as whatever <em>@self</em> happens to resolve to, your alias options will make it into the definition for <em>@self</em>. So here&rsquo;s the alternate solution:</p>

<p>``` php ~/.drush/clientsite.aliases.drush.php
$aliases[&lsquo;localdev&rsquo;] = array(
  &lsquo;root&rsquo; => &lsquo;/Users/cvertesi/Sites/clientsite&rsquo;,
  &lsquo;uri&rsquo; => &lsquo;default&rsquo;,
  &lsquo;path-aliases&rsquo; => array(</p>

<pre><code>'%dump-dir' =&gt; '/home/cvertesi/.drush/db_dumps',
</code></pre>

<p>  ),
);
```</p>

<p>There&rsquo;s just one, obscure caveat with the latter method: somewhere in the alias merging process, BASH aliases are lost. That means that &lsquo;~&rsquo; stops resolving to your home directory, and you have to write it out (as I did above).</p>

<p>Have fun!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SSH Lifehacks to Make Your SSH Life Easy]]></title>
    <link href="http://ohthehugemanatee.org/blog/2013/12/20/ssh-lifehacks-to-make-your-ssh-life-easy/"/>
    <updated>2013-12-20T16:29:27+01:00</updated>
    <id>http://ohthehugemanatee.org/blog/2013/12/20/ssh-lifehacks-to-make-your-ssh-life-easy</id>
    <content type="html"><![CDATA[<p>If you use SSH every day, or even every other day, this post is for you. We&rsquo;re going to walk through some of the more convenient options available to you in your global SSH configuration to make your life easier and faster.</p>

<p>First of all, you should know that SSH has a great configuration file, typically located in your home directory at <em>.ssh/config</em>. Everything we&rsquo;re talking about today belongs there. There&rsquo;s tons of stuff you can do with it, and I&rsquo;m only going to scratch the surface.</p>

<h2>Re-Using Existing Connections</h2>

<p>Every time you SSH into a server, it opens a new connection and authenticates all over again, right? That&rsquo;s slow, and it&rsquo;s a PITA if you have to type a password for your private key or just to access the server. You&rsquo;re already connected once, so why not reuse the same connection? Just add these lines to your <em>.ssh/config</em>:</p>

<p>```</p>

<h1>Re-use existing connections instead of opening new ones</h1>

<p>ControlMaster auto
ControlPath /tmp/ssh_mux<em>%h</em>%p<em>%r</em>%l
```</p>

<p>This tells SSH to use a socket file for each connection, and to name the file in a way that&rsquo;s unique for each combination of host, port, remote username, and local hostname. That is to say, if you&rsquo;re connecting to the same host/port with the same username and from the same local hostname, it will automatically re-use the existing connection.</p>

<p><strong>Result: Multiple SSH connections are WAY faster.</strong></p>

<h2>Host shortcuts and definitions</h2>

<p>You can easily define commonly used connections with shortcuts. So instead of typing <code>ssh -i ~/.ssh/ohthehugemanatee.pem -P 2222 ohthehugemanatee@live.ohthehugemanatee.org</code>, you can just type <code>ssh live</code>.</p>

<p>``` bash
Host live
  Hostname live.ohthehugemanatee.org
  Port 2222
  User ohthehugemanatee
  IdentityFile ~/.ssh/ohthehugemanatee.pem</p>

<p>```</p>

<h2>SSH Agent Forwarding &ndash; One key to rule them all</h2>

<p>Normally when you want to access a resource from multiple machines, you have to generate a public/private keypair on each machine. A great example is a git repo: if I want to access the same git repository on my localhost, the staging server, and the live server, I will have to generate three keypairs and grant access to all three. Talk about a pain in the ass!</p>

<p>Fortunately there&rsquo;s a much easier way to do this, which is available in almost all distributed versions of openssh: SSH agent forwarding. With agent forwarding enabled, your private key from one machine becomes available for each subsequent machine you connect to. In other words, if you have one private key on your localhost, and SSH into another server, your local private key will be available if you want to SSH (or use git) from that server. Automatically. And only while you&rsquo;re connected, so it&rsquo;s safe.</p>

<p><code>
ForwardAgent yes
</code></p>

<p>Just drop that in your Host definition (per above), and if the remote host supports it it will work automatically. It is not recommended to set this option globally, because your private key is actually very sensitive stuff, and you don&rsquo;t want to accidentally forward your key to an untrusted server. But feel free to enable it on every host where it&rsquo;s convenient!</p>

<h2>Bonus material</h2>

<p>Technically these items aren&rsquo;t about <em>.ssh/config</em>, but they&rsquo;re still so convenient I had to share them.</p>

<p>Note that the same improvements to your SSH will also apply to your use of SCP and other SSH tools. So that brutally long and painful rsync-over-ssh command that you&rsquo;ve had to type a thousand times, can now just use &ldquo;live&rdquo; as a shortcut. Git, too. It&rsquo;s a great relief to be able to type <code>scp ~/Sites/index.php live:~</code>. It&rsquo;s just so much more readable!</p>

<p>But if you want to get <strong>even lazier</strong>, you&rsquo;re going to love the next tip. Why ssh in at all, when you can just mount the remote directory somewhere convenient on your local system? Let&rsquo;s use that Host alias above, and mount the remote web root (<em>/var/www/html</em>) at <em>~/live-environment</em>. It&rsquo;s much more convenient there, after all.</p>

<p><code>sshfs live:/var/www/html ~/live</code></p>

<p>Note that this no longer works in OSX without installing <a href="http://osxfuse.github.io/">OSX Fuse</a>. Still, the 5 minute download and install process is totally worth it for this easy SSH lifehack.</p>

<p>Or is that not lazy enough for you? &ldquo;Do I really have to mount the whole directory?&rdquo; I hear you cry! &ldquo;I wanna just edit one file&hellip;&rdquo; Well have I got the tip for you! For the ultimately lazy, <a href="http://www.vim.org/">vim</a> supports editing files directly over SSH. Try it out:</p>

<p><code>vim scp://live/var/www/html/index.php</code></p>

<p>Think about it: before you read this post, that would have taken you several long-winded commands to type. Your arthritic fingers can thank me later.</p>
]]></content>
  </entry>
  
</feed>
