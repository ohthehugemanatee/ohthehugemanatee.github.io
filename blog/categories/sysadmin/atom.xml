<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Sysadmin | Oh, The Huge Manatee]]></title>
  <link href="http://ohthehugemanatee.org/blog/categories/sysadmin/atom.xml" rel="self"/>
  <link href="http://ohthehugemanatee.org/"/>
  <updated>2018-12-27T17:18:57+01:00</updated>
  <id>http://ohthehugemanatee.org/</id>
  <author>
    <name><![CDATA[Campbell Vertesi (ohthehugemanatee)]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Optimizing Data Transfer Speeds]]></title>
    <link href="http://ohthehugemanatee.org/blog/2018/12/27/optimizing-data-transfer-speeds/"/>
    <updated>2018-12-27T10:30:23+01:00</updated>
    <id>http://ohthehugemanatee.org/blog/2018/12/27/optimizing-data-transfer-speeds</id>
    <content type="html"><![CDATA[<p>One of my holiday projects was to set up my home &ldquo;data warehouse.&rdquo; Ever since <a href="https://www.dropboxforum.com/t5/Syncing-and-uploads/Linux-Dropbox-client-warn-me-that-it-ll-stop-syncing-in-Nov-why/m-p/290065/highlight/true#M42255">Dropbox killed modern Linux filesystem support</a> I&rsquo;ve been using (and loving) <a href="https://nextcloud.com/">Nextcloud</a> from my home. It backs up to an encrypted <a href="https://www.duplicati.com/">Duplicati</a> store on <a href="https://azure.microsoft.com/en-us/services/storage/blobs/">Azure blob store</a>, so that&rsquo;s offsite backups taken care of. But it was time to knit all my various drives together into a single RAID data warehouse. The only problem: how to transfer my 2 terabytes (rounded to make the math in the post easier) of data, without nasty downtime during the holidays?</p>

<p>A local network transfer is the fastest, with the least downtime. I have a switched gigabit network in my house, and all my servers are hard wired. That&rsquo;s about 125 megabytes per second; a theoretical 5 hours to transfer everything. Not bad! Start up an rsync and I&rsquo;m all done! So I kicked it off and went to bed:</p>

<p><code>bash
$ ssh nextcloud.vert
$ rsync -axz /media/usbdrive/ warehouse:/mnt/storage/ --log-file=transfer-to-warehouse.log &amp;
</code></p>

<p>I woke up in the morning with the excitement of a kid on Christmas. Everything should be done, right?</p>

<p><code>bash
$ ssh warehouse df -h |grep md0
/dev/md0        2.7T  501G  2.1T  20% /mnt/storage
$
</code>
Wait, what? How had it only transferred 500 gigabytes overnight? Including time for <em>Doctor Who</em> and breakfast, that was only 1 Megabit per second! I knew it was time to play everyone&rsquo;s favorite game: <em>&ldquo;where&rsquo;s the bottleneck?</em></p>

<p>I guess it could be rsync scanning all those small files. If that&rsquo;s the case, we&rsquo;ll see high CPU usage, and even higher load numbers (as processes are I/O blocked):</p>

<p>``` bash
$ ssh nextcloud
$ top</p>

<p>top &ndash; 08:22:27 up 10:26,  1 user,  load average: 1.20, 1.34, 1.33
Tasks: 170 total,   2 running, 106 sleeping,   0 stopped,   0 zombie
%Cpu(s): 28.0 us,  2.1 sy,  0.0 ni, 69.5 id,  0.1 wa,  0.0 hi,  0.3 si,  0.0 st
KiB Mem : 16330372 total,   180568 free,   657104 used, 15492700 buff/cache
KiB Swap:  4194300 total,  4162556 free,    31744 used. 15300068 avail Mem</p>

<p>  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                <br/>
 8755 ohthehu+  20   0  130572  58456   2672 R  99.0  0.4 513:14.75 rsync                                                                                  <br/>
 8756 ohthehu+  20   0   49596   6648   5152 S  16.9  0.0  92:12.29 ssh
&hellip;
```</p>

<p>OK, let&rsquo;s kill the transfer and start again using a single large, piped tarball. No more small file scans!</p>

<p><code>bash
$ ssh nextcloud
$ cd /media/bigdrive &amp;&amp; tar cf - . | ssh warehouse "cd /mnt/storage &amp;&amp; tar xpvf -"
</code></p>

<p>That helps, but we&rsquo;re still compressing lots of data unnecessarily (most of my data is already compressed), and encrypting it, too. We can improve it with a lightweight ssh cipher and disabled compression:</p>

<p><code>bash
$ ssh nextcloud
$ cd /media/bigdrive &amp;&amp; tar cf - . | ssh -o Compression=no -c chacha20-poly1305@openssh.com warehouse "cd /mnt/storage &amp;&amp; tar xpf -"
</code></p>

<p>That chacha20-poly1305 is a very fast cipher indeed &ndash; faster than the old arcfour cipher we used to use in this case. But SSH still puts extra work on the CPU. So let&rsquo;s remove it completely from the equation and just use netcat.</p>

<p>``` bash
$ ssh nextcloud cd /media/bigdrive &amp;&amp; tar cf &ndash; . | pv | nc -l -q 5 -p 9999</p>

<h1>in a separate terminal</h1>

<p>$ ssh warehouse cd /mnt/storage &amp;&amp; nc nextcloud 9999 | pv | tar -xf &ndash;
```</p>

<p>Transfer speeds now average about 61 megabytes per second. That&rsquo;s fast enough to kick in the law of diminishing returns on my optimization effort: this will take about 8 hours to transfer if I keep it running. I had to pause work for an hour; now if I spend another hour on this, it has to shave more than 25% off my transfer time to finish any earlier tonight. I&rsquo;m not confident I can beat those numbers.</p>

<p>Still &ndash; What happened to my 125 theoretical megabytes per second? Here are the culprits I suspect &ndash; and can&rsquo;t really do anything about:</p>

<ul>
<li><p><strong>Slow disk</strong>: We are writing to a software RAID5 array of old drives. In my head I was using the channel width of SATA-II for my calculations. In reality, and especially on spinning metal, write speeds are much slower. I looked up my component disks on <a href="userbenchmark.com">userbenchmark.com</a>, and the slowest member has an average sustained sequential write speed of 69 MB/s. This is very likely my first bottleneck. At most I can only use half of my available bandwidth.</p></li>
<li><p><strong>TCP</strong>: After replacing all my drives with SSDs, TCP is the next culprit I would go after. The protocol technically only has about 6% of overhead, but it also dynamically seeks the maximum send rate through <a href="https://en.wikipedia.org/wiki/TCP_congestion_control">TCP Congestion Control</a>. It keeps trying to send &ldquo;just a little faster&rdquo;, until the number of unacknowledged packets exceeds a threshold. Then it backs off to 50%, and goes back to &ldquo;just a little faster&rdquo; mode. This loop means your practical speed with a TCP stream is about 75% of the pipe&rsquo;s theoretical maximum. Think of it like John Cleese offering just one more &ldquo;wafer thin&rdquo; packet. I considered using UDP to avoid this, but I actually <em>want</em> the error-checking in TCP. Probably the best solution is <a href="https://github.com/LabAdvComp/UDR">something esoteric like UDR</a>.</p></li>
<li><p><strong>Slow CPU</strong>: This is the last bottleneck here. <em>Warehouse</em> is an old Intel Core2 Duo I had lying around the house. Untar and netcat aren&rsquo;t exactly CPU hungry beasts, but at some point there IS a maximum. If you believe the FreeNAS folks, a fileserver needs an i5 and 8 gigs of RAM for basic functionality. I haven&rsquo;t found that to be the case, but then I&rsquo;m not using RAID-Z, either.</p></li>
</ul>


<p>I&rsquo;m happy with the outcome here. I have another drive to copy later, with another terabyte. I&rsquo;m considering removing that slowest drive from my RAID array, since the next-slowest one is almost 50% faster. Then I can copy to the array while it&rsquo;s in degraded mode, and re-add the slowpoke afterwards. We&rsquo;ll see.</p>

<p>Happy holidays!</p>

<h3>Appendix: easy performance testing</h3>

<p>If you&rsquo;re working on a similar problem for yourself, you might find these performance testing commands helpful. The idea is to tease apart each component of the transfer. There are better, more detailed, dedicated tools for each of these, but in a game of &ldquo;find the bottleneck&rdquo; you really only need quick and dirty validation. Fun fact: the command <em>dd</em> is actually short for <strong>D</strong>own and <strong>D</strong>irty. Well it should be, at any rate.</p>

<p><strong>Read speed (on the source</strong> is easy: hand an arbitrary large file to dd, and write down the numbers it gives.</p>

<p><code>bash
$ dd if=large-file.tar.bz2 of=/dev/null bs=1M
1021317200 bytes (1 GB) copied, 3.9888 s, 256 MB/s
</code></p>

<p><strong>Network speed</strong> can be tested by netcatting a gigabyte of zeros from one machine to the other.</p>

<p>``` bash</p>

<h1>On the receiving machine, open a port to /dev/null</h1>

<p>$ nc -vvlnp 12345 >/dev/null</p>

<h1>On the sending machine, send a gig of zeroes to that port</h1>

<p>$ dd if=/dev/zero bs=1M count=1K | nc -vvn 192.168.1.50 12345
Connection to 192.168.1.50 12345 port [tcp/*] succeeded!
1024+0 records in
1024+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 11.7811 s, 91.1 MB/s</p>

<h1>Remember, 8 bits to a byte!</h1>

<p>$ echo &ldquo;$(bc -l &lt;&lt;&lt; 91*8) Megabits&rdquo;
728 Megabits
```</p>

<p><strong>Write speed on the destination</strong> can be tested with dd, too:</p>

<p><code>bash
$ dd if=/dev/zero bs=1M count=1024 of=/mnt/storage/test.img
1024+0 records in
1024+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 55.3836 s, 19.4 MB/s
</code></p>

<p>(note: these tests were run while the copy was happening on warehouse. Your numbers should be better than this!)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What Crell Doesn't Want You to Know: How to Automate Letsencrypt on platform.sh]]></title>
    <link href="http://ohthehugemanatee.org/blog/2017/02/21/what-crell-doesnt-want-you-to-know-how-to-automate-letsencrypt-on-platform-dot-sh/"/>
    <updated>2017-02-21T22:33:08+01:00</updated>
    <id>http://ohthehugemanatee.org/blog/2017/02/21/what-crell-doesnt-want-you-to-know-how-to-automate-letsencrypt-on-platform-dot-sh</id>
    <content type="html"><![CDATA[<p>If you believe the <a href="https://docs.platform.sh/development/going-live.html#prerequisites">docs</a> and the <a href="https://twitter.com/damz/status/672559665377501184">twitters</a>, there is no way to automate <a href="https://letsencrypt.org/">letsencrypt</a> certificates updates on <a href="https://platform.sh/">platform.sh</a>. You have to create the certificates manually, upload them manually, and maintain them manually.</p>

<p>But as readers of this blog know, the docs are only the start of the story. I&rsquo;ve really enjoyed working with platform.sh with one of my private clients, and I couldn&rsquo;t believe that with all the flexibility &ndash; all the POWER &ndash; letsencrypt was really out of reach. I found a few attempts to script it, and one really great <a href="https://gitlab.com/snippets/27467">snippet on gitlab</a>. But no one had ever really synthesized this stuff into an easy howto. So here we go.</p>

<h3>1) Add some writeable directories where platform.sh CLI and letsencrypt need them.</h3>

<p>Normally when Platform deploys your application, it puts it all in a read-only filesystem. We&rsquo;re going to mount some special directories read-write so all the letsencrypt/platform magic can work.</p>

<p>Edit your application&rsquo;s <code>.platform.app.yaml</code> file, and find the <code>mounts:</code> section. At the bottom, add these three lines. Make sure to match the indents with everything else under the <code>mounts:</code> section!</p>

<p>```</p>

<pre><code>"/web/.well-known": "shared:files/.well-known"
"/keys": "shared:files/keys"
"/.platformsh": "shared:files/.platformsh"
</code></pre>

<p>```</p>

<p>Let&rsquo;s walk through each of these:</p>

<ul>
<li>/web/.well-known: In order to confirm that you actually control example.com, letsencrypt drops a file somewhere on your website, and then tries to fetch it. This directory is where it&rsquo;s going to do the drop and fetch. My webroot is <code>web</code>, you should change this to match your own environment. You might use <code>public</code> or <code>www</code> or something.</li>
<li>/keys: You have to store your keyfiles SOMEWHERE. This is that place.</li>
<li>/.platformsh: Your master environment needs a bit of configuration to be able to login to platform and update the certs on your account. This is where that will go.</li>
</ul>


<h3>2) Expose the .well-known directory to the Internet</h3>

<p>I mentioned above that letsencrypt test your control over a domain by creating a file which it tries to fetch over the Internet. We already created the writeable directory where the scripts can drop the file, but platform.sh (wisely) defaults to hide your directories from the Internet. We&rsquo;re going to add some configuration to the &ldquo;web&rdquo; app section to expose this .well-known directory. Find the <code>web:</code> section of your <code>.platform.app.yaml</code> file, and the <code>locations:</code> section under that. At the bottom of that section, add this:</p>

<p>```</p>

<pre><code>  '/.well-known':
        # Allow access to all files in the public files directory.
        allow: true
        expires: 5m
        passthru: false
        root: 'web/.well-known'
        # Do not execute PHP scripts.
        scripts: false
</code></pre>

<p>```</p>

<p>Make sure you match the indents of the other location entries! In my (default) <code>.platform.app.yaml</code> file, I have 8 spaces before that <code>'/.well-known':</code> line. Also note that the <code>root:</code> parameter there also uses my webroot directory, so adjust that to fit your environment.</p>

<h3>3) Download the binaries you need during the application &ldquo;build&rdquo; phase</h3>

<p>In order to do this, we&rsquo;re going to need to have the platform.sh CLI tool, and a let&rsquo;s encrypt CLI tool called lego. We&rsquo;ll download them during the &ldquo;build&rdquo; phase of your application. Still in the <code>platform.app.yaml</code> file, find the <code>hooks:</code> section, and the <code>build:</code> section under that. Add these steps to the bottom of the build:</p>

<p>```</p>

<pre><code>  cd ~
  curl -sL https://github.com/xenolf/lego/releases/download/v0.3.1/lego_linux_amd64.tar.xz | tar -C .global/bin -xJ --strip-components=1 lego/lego
  curl -sfSL -o .global/bin/platform.phar https://github.com/platformsh/platformsh-cli/releases/download/v3.12.1/platform.phar
</code></pre>

<p>```</p>

<p>We&rsquo;re just downloading reasonably recent releases of our two tools. If anyone has a better way to get the latest release of either tool, please let me know. Otherwise we&rsquo;re stuck keeping this up to date manually.</p>

<h3>4) Configure the platform.sh CLI</h3>

<p>In order to configure the platform.sh CLI on your server, we have to deploy the changes from steps 1-3. Go ahead and do that now. I&rsquo;ll wait.</p>

<p>Now connect to your platform environment via SSH (<code>platform ssh -e master</code> for most of us). First we&rsquo;ll add a config file for platform. Edit a file in <code>.platformsh/config.yaml</code> with the editor of choice. You don&rsquo;t have to use vi, but it will win you some points with me. Here are the contents for that file:</p>

<p>```
updates:</p>

<pre><code>check: false
</code></pre>

<p>api:</p>

<pre><code>token_file: token
</code></pre>

<p>```</p>

<p>Pretty straightforward: this tells platform not to bother updating the CLI tool automatically (it can&rsquo;t &ndash; read-only filesystem, remember?). It then tells it to login using an API token, which it can find in the file <code>.platformsh/token</code>. Let&rsquo;s create that file next.</p>

<p>Log into the platform.sh web UI (you can launch it with <code>platform web</code> if you&rsquo;re feeling sassy), and navigate to your account settings > api tokens. That&rsquo;s at <code>https://accounts.platform.sh/user/12345/api-tokens</code> (with your own user ID of course). Add an API token, and copy its value into <code>.platformsh/token</code> on the environment we&rsquo;re working on. The token should be the only contents of that file.</p>

<p>Now let&rsquo;s test it by running <code>php /app/.global/bin/platform.phar auth:info</code>. If you see your account information, congratulations! You have a working platform.sh CLI installed.</p>

<h3>5) Request your first certificate by hand</h3>

<p>Still SSH'ed into that environment, let&rsquo;s see if everything works.</p>

<p><code>
lego --email="support@example.com" --domains="www.example.com" --webroot=/app/public/ --path=/app/keys/ -a run
csplit -f /app/keys/certificates/www.example.com.crt- /app/keys/certificates/www.example.com.crt '/-----BEGIN CERTIFICATE-----/' '{1}' -z -s
php /app/.global/bin/platform.phar domain:update -p $PLATFORM_PROJECT --no-wait --yes --cert /app/keys/certificates/www.example.com.crt-00 --chain /app/keys/certificates/www.example.com.crt-01 --key /app/keys/certificates/www.example.com.key example.com
</code></p>

<p>This is three commands: register the cert with letsencrypt, then split the resulting file into it&rsquo;s components, then register those components with platform.sh. If you didn&rsquo;t get any errors, go ahead and test your site &ndash; it&rsquo;s got a certificate! (yay)</p>

<h3>6) Set up automatic renewals on cron</h3>

<p>Back to <code>.platform.app.yaml</code>, look for the <code>crons:</code> section. If you&rsquo;re running drupal, you probably have a drupal cronjob in there already. Add this one at the bottom, matching indents as always.</p>

<p>```</p>

<pre><code>letsencrypt:
    spec: '0 0 1 * *'
    cmd: '/bin/sh /app/scripts/letsencrypt.sh'
</code></pre>

<p>```</p>

<p>Now let&rsquo;s create the script. Add the file <code>scripts/letsencrypt.sh</code> to your repo, with this content:</p>

<p>``` bash</p>

<h1>!/usr/bin/env bash</h1>

<h1>Checks and updates the letsencrypt HTTPS cert.</h1>

<p>set -e</p>

<p>if [ &ldquo;$PLATFORM_ENVIRONMENT&rdquo; = &ldquo;master-7rqtwti&rdquo; ]
  then</p>

<pre><code># Renew the certificate
lego --email="example@example.org" --domains="example.org" --webroot=/app/web/ --path=/app/keys/ -a renew
# Split the certificate from any intermediate chain
csplit -f /app/keys/certificates/example.org.crt- /app/keys/certificates/example.org.crt '/-----BEGIN CERTIFICATE-----/' '{1}' -z -s
# Update the certificates on the domain
php /app/.global/bin/platform.phar domain:update -p $PLATFORM_PROJECT --no-wait --yes --cert /app/keys/certificates/example.org.crt-00 --chain /app/keys/certificates/example.org.crt-01 --key /app/keys/certificates/example.org.key example.org
</code></pre>

<p>fi
```</p>

<p>Obviously you should replace all those <code>example.org</code>s and email addresses with your own domain. Make the file executable with <code>chmod u+x scripts/letsencrypt.sh</code>, commit it, and push it up to your platform.sh environment.</p>

<h3>7) Send a bragging email to Crell</h3>

<p>Technically this isn&rsquo;t supposed to be possible, but YOU DID IT! Make sure to rub it in.</p>

<p><img class="center" src="/images/larry-garfield.jpg" title="&ldquo;Larry is waiting to hear from you. (photo credit Jesus Manuel Olivas)&rdquo;" ></p>

<p>Good luck!</p>

<p>PS &ndash; I&rsquo;m just gonna link one more time to the guy whose snippet made this all possible: <a href="https://www.drupal.org/u/hanoii">Ariel Barreiro</a> did the hardest part of this. I&rsquo;m grateful that he made his notes public!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Drush Self Aliases]]></title>
    <link href="http://ohthehugemanatee.org/blog/2014/01/10/drush-self-aliases/"/>
    <updated>2014-01-10T09:22:01+01:00</updated>
    <id>http://ohthehugemanatee.org/blog/2014/01/10/drush-self-aliases</id>
    <content type="html"><![CDATA[<p>I ran into an interesting problem with the drush <em>@self</em> alias today. I wanted to pull a fresh copy of the DB down from a client&rsquo;s live site to my local development copy. Should be as easy as <em>drush sql-sync @clientsite.live @self</em>, right? I&rsquo;ve done this a thousand times before.</p>

<p>And I&rsquo;ve also ignored the warning message every time before, but today I thought I&rsquo;d check it out:</p>

<blockquote><p>WARNING:  Using temporary files to store and transfer sql-dump.  It is recommended that you specify &mdash;source-dump and &mdash;target-dump options on the command line, or set &lsquo;%dump&rsquo; or &lsquo;%dump-dir&rsquo; in the path-aliases section of your site alias records. This facilitates fast file transfer via rsync.</p></blockquote>

<p>There are actually two possible solutions to this warning (that I can think of), and they illustrate some of the useful &ldquo;power user&rdquo; features of Drush that any frequent user should be aware of.</p>

<p>The warning is there because drush would <em>prefer</em> to rsync the DB dump from site1 to site2, rather than a one time copy. Rsync has lots of speed improvements, not the least being diff transfer. When transferring an updated copy of a file which already exists at the destination, rsync will only send over the changes rather than the whole file. This is pretty useful if you&rsquo;re dealing with a large, text based file like an SQL dump &ndash; especially one that you&rsquo;ll be transferring often. In order to use this efficient processing though, Drush needs to know a safe path where it can store the DB dump in each location.</p>

<p>First we&rsquo;ll add the <em>%dump-dir%</em> attribute to our alias for clientsite:</p>

<p>``` php ~/.drush/clientsite.aliases.drush.php
&lt;?php
// Site clientsite, environment live
$aliases[&lsquo;live&rsquo;] = array(
  &lsquo;parent&rsquo; => &lsquo;@parent&rsquo;,
  &lsquo;site&rsquo; => &lsquo;clientsite&rsquo;,
  &lsquo;env&rsquo; => &lsquo;live&rsquo;,
  &lsquo;root&rsquo; => &lsquo;/var/www/example.com/public_html&rsquo;,
  &lsquo;remote-host&rsquo; => &lsquo;example.com&rsquo;,
  &lsquo;remote-user&rsquo; => &lsquo;cvertesi&rsquo;,
  &lsquo;path-aliases&rsquo; => array(</p>

<pre><code>'%dump-dir' =&gt; '/home/cvertesi/.drush/db_dumps',
</code></pre>

<p>  ),
);
```</p>

<p>Notice that <em>%dump-dir</em> actually goes in a special sub-array for <em>path-aliases</em>. This is very likely the only time you&rsquo;ll need to use that section, since most everything else in there is auto-detected. This is the directory on the remote side where drush will store the dump.</p>

<p>Our options come in with the <em>@self</em> alias. In a local dev environment, the most common way to handle this is in your <em>drushrc.php</em> file:</p>

<p><code>php ~/.drush/drushrc.php
$options['dump-dir'] = '~/.drush/db_dumps';
</code></p>

<p>But this won&rsquo;t work for all cases. You can also take advantage of Drush&rsquo;s alias handling by creating a site alias with the settings you want, and letting Drush merge those settings into <em>@self</em>. When Drush builds its' cache of path aliases, it uses the site path as the cache key (for local sites only). That means that if you have a local alias with the same path as whatever <em>@self</em> happens to resolve to, your alias options will make it into the definition for <em>@self</em>. So here&rsquo;s the alternate solution:</p>

<p>``` php ~/.drush/clientsite.aliases.drush.php
$aliases[&lsquo;localdev&rsquo;] = array(
  &lsquo;root&rsquo; => &lsquo;/Users/cvertesi/Sites/clientsite&rsquo;,
  &lsquo;uri&rsquo; => &lsquo;default&rsquo;,
  &lsquo;path-aliases&rsquo; => array(</p>

<pre><code>'%dump-dir' =&gt; '/home/cvertesi/.drush/db_dumps',
</code></pre>

<p>  ),
);
```</p>

<p>There&rsquo;s just one, obscure caveat with the latter method: somewhere in the alias merging process, BASH aliases are lost. That means that &lsquo;~&rsquo; stops resolving to your home directory, and you have to write it out (as I did above).</p>

<p>Have fun!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SSH Lifehacks to Make Your SSH Life Easy]]></title>
    <link href="http://ohthehugemanatee.org/blog/2013/12/20/ssh-lifehacks-to-make-your-ssh-life-easy/"/>
    <updated>2013-12-20T16:29:27+01:00</updated>
    <id>http://ohthehugemanatee.org/blog/2013/12/20/ssh-lifehacks-to-make-your-ssh-life-easy</id>
    <content type="html"><![CDATA[<p>If you use SSH every day, or even every other day, this post is for you. We&rsquo;re going to walk through some of the more convenient options available to you in your global SSH configuration to make your life easier and faster.</p>

<p>First of all, you should know that SSH has a great configuration file, typically located in your home directory at <em>.ssh/config</em>. Everything we&rsquo;re talking about today belongs there. There&rsquo;s tons of stuff you can do with it, and I&rsquo;m only going to scratch the surface.</p>

<h2>Re-Using Existing Connections</h2>

<p>Every time you SSH into a server, it opens a new connection and authenticates all over again, right? That&rsquo;s slow, and it&rsquo;s a PITA if you have to type a password for your private key or just to access the server. You&rsquo;re already connected once, so why not reuse the same connection? Just add these lines to your <em>.ssh/config</em>:</p>

<p>```</p>

<h1>Re-use existing connections instead of opening new ones</h1>

<p>ControlMaster auto
ControlPath /tmp/ssh_mux<em>%h</em>%p<em>%r</em>%l
```</p>

<p>This tells SSH to use a socket file for each connection, and to name the file in a way that&rsquo;s unique for each combination of host, port, remote username, and local hostname. That is to say, if you&rsquo;re connecting to the same host/port with the same username and from the same local hostname, it will automatically re-use the existing connection.</p>

<p><strong>Result: Multiple SSH connections are WAY faster.</strong></p>

<h2>Host shortcuts and definitions</h2>

<p>You can easily define commonly used connections with shortcuts. So instead of typing <code>ssh -i ~/.ssh/ohthehugemanatee.pem -P 2222 ohthehugemanatee@live.ohthehugemanatee.org</code>, you can just type <code>ssh live</code>.</p>

<p>``` bash
Host live
  Hostname live.ohthehugemanatee.org
  Port 2222
  User ohthehugemanatee
  IdentityFile ~/.ssh/ohthehugemanatee.pem</p>

<p>```</p>

<h2>SSH Agent Forwarding &ndash; One key to rule them all</h2>

<p>Normally when you want to access a resource from multiple machines, you have to generate a public/private keypair on each machine. A great example is a git repo: if I want to access the same git repository on my localhost, the staging server, and the live server, I will have to generate three keypairs and grant access to all three. Talk about a pain in the ass!</p>

<p>Fortunately there&rsquo;s a much easier way to do this, which is available in almost all distributed versions of openssh: SSH agent forwarding. With agent forwarding enabled, your private key from one machine becomes available for each subsequent machine you connect to. In other words, if you have one private key on your localhost, and SSH into another server, your local private key will be available if you want to SSH (or use git) from that server. Automatically. And only while you&rsquo;re connected, so it&rsquo;s safe.</p>

<p><code>
ForwardAgent yes
</code></p>

<p>Just drop that in your Host definition (per above), and if the remote host supports it it will work automatically. It is not recommended to set this option globally, because your private key is actually very sensitive stuff, and you don&rsquo;t want to accidentally forward your key to an untrusted server. But feel free to enable it on every host where it&rsquo;s convenient!</p>

<h2>Bonus material</h2>

<p>Technically these items aren&rsquo;t about <em>.ssh/config</em>, but they&rsquo;re still so convenient I had to share them.</p>

<p>Note that the same improvements to your SSH will also apply to your use of SCP and other SSH tools. So that brutally long and painful rsync-over-ssh command that you&rsquo;ve had to type a thousand times, can now just use &ldquo;live&rdquo; as a shortcut. Git, too. It&rsquo;s a great relief to be able to type <code>scp ~/Sites/index.php live:~</code>. It&rsquo;s just so much more readable!</p>

<p>But if you want to get <strong>even lazier</strong>, you&rsquo;re going to love the next tip. Why ssh in at all, when you can just mount the remote directory somewhere convenient on your local system? Let&rsquo;s use that Host alias above, and mount the remote web root (<em>/var/www/html</em>) at <em>~/live-environment</em>. It&rsquo;s much more convenient there, after all.</p>

<p><code>sshfs live:/var/www/html ~/live</code></p>

<p>Note that this no longer works in OSX without installing <a href="http://osxfuse.github.io/">OSX Fuse</a>. Still, the 5 minute download and install process is totally worth it for this easy SSH lifehack.</p>

<p>Or is that not lazy enough for you? &ldquo;Do I really have to mount the whole directory?&rdquo; I hear you cry! &ldquo;I wanna just edit one file&hellip;&rdquo; Well have I got the tip for you! For the ultimately lazy, <a href="http://www.vim.org/">vim</a> supports editing files directly over SSH. Try it out:</p>

<p><code>vim scp://live/var/www/html/index.php</code></p>

<p>Think about it: before you read this post, that would have taken you several long-winded commands to type. Your arthritic fingers can thank me later.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SSL Certificates Demystified]]></title>
    <link href="http://ohthehugemanatee.org/blog/2013/06/20/ssl-certificates-demystified/"/>
    <updated>2013-06-20T00:00:00+02:00</updated>
    <id>http://ohthehugemanatee.org/blog/2013/06/20/ssl-certificates-demystified</id>
    <content type="html"><![CDATA[<p>It seems like any time anyone has to configure a server for SSL/TLS, they treat it like some kind of voodoo magic. And I guess it can seem that way: if you get your certificates in the right formats from your certificate authority, you basically just drop them into Apache/NGINX/whatever and let it go.&nbsp;</p>


<p>But what happens if those files&nbsp;<strong>aren't</strong> in the right format by default? And what if you actually want to understand what's going on with your server?</p>


<p>The truth is, encryption is not terribly difficult to understand. It's a little tricky to explain, and the maths in the nitty gritty of it can get really intense, but a high level understanding is important for anyone who works with SSL.&nbsp;</p>


<div>The basis for SSL is <a href="https://en.wikipedia.org/wiki/Public_key_cryptography">public-key</a> (asymmetrical) cryptography. The idea is that you create a pair of files that have matching abilities:</div>


<div>&nbsp;</div>


<div>*<strong> the "public" file can&nbsp;encrypt&nbsp;content</strong> so that the matching "private" file must be used to decrypt it. It can also <strong>verify the signature</strong> of the "private" file.</div>


<div>&nbsp;</div>


<div>* <strong>the "private" file can&nbsp;decrypt&nbsp;content</strong> that was encrypted by the matching "public" file. It can also <strong>leave a unique digital signature</strong> which can be verified by the matching "public" file.</div>


<div>&nbsp;</div>


<div>If you've ever used the <a href="https://en.wikipedia.org/wiki/Pretty_Good_Privacy">PGP encryption</a> system, that's basically how it works. You publish your public key in a publicly available directory, and keep your private key to yourself. That way, anyone can use your public key to encrypt content for you, and only you can decrypt it.</div>


<div>&nbsp;</div>


<div>In the context of web communications, this model is taken to the next level. We use the encryption aspect of it, but we also use those digital signatures. Here's the process.</div>


<div>&nbsp;</div>


<div>1) You create this "key pair" for yourself, and hide the private key away somewhere. Optionally the private key can be password-protected.</div>


<div>&nbsp;</div>


<div>2) You create a Certificate Request file (.CSR, usually). A Certificate Request is just a copy of your Public Key, with some identifying information about your organization. The most important piece of this information is the domain name this Request is for. You submit your CSR file to a Certificate Authority (abbreviated as "CA) company like <a href="https://www.rapidssl.com/">RapidSSL</a>, or any number of other Certificate Authorities out there.</div>


<div>&nbsp;</div>


<div>3) The CA verifies that you really own the domain name specified in the CSR. How they verify that is up to the CA. Maybe they email the owner of the domain, maybe they ask you to modify a DNS record, maybe they just take you on your word (hopefully not!). Whatever they do, when the CA is satisfied that you really do own that domain name, they send you back your Certificate Request file. This version of the file is signed by the CA's own private key, and that difference makes it the actual "SSL certificate" file. If you've been keeping score at home, the SSL certificate is simply your public key with your organizational information attached, and your CA's signature.&nbsp;</div>


<div>&nbsp;</div>


<div>Great, now people can send you encrypted messages, right? Well, sorta. How do people know they can trust the CA? Maybe that CA is just some yahoo who lives up the hall, or your great aunt Edna. How can I trust Edna to vouch for you?&nbsp;</div>


<div>&nbsp;</div>


<div>4) The solution is a "chain of authority" file. This is just your CA's public key (which the browser needs to check that signature on your SSL certificate, remember?). What makes this public key special is that it's ALSO signed by whoever granted your CA permission to be a CA, and by whoever granted THAT person the permission to be a CA, all the way back to one of the "root" certificate authorities, whose public keys are bundled with every browser. (there are about 30 of them). So the chain file from aunt Edna might contain signatures from her daughter Annie, Annie's company AnnieCA, and GeoTrust, who granted AnnieCA the authority to give out certificates. GeoTrust is a root CA, so your browser already knows they're trusted. &nbsp;</div>


<div>&nbsp;</div>


<div>So when a web browser connects to your server, your server sends out its SSL certificate file (ie the server's public key with Edna's "Seal of Authenticity" on it). The browser can then encrypt communications to send to your server, and your server can decrypt them with its private key. The browser can also check on Edna's Seal of Authenticity, because there's a chain of other CAs who have signed Edna's certificate, right back to one of the CAs that the browser already trusts.</div>


<div>&nbsp;</div>


<div>With a one-way secure channel open, and with confidence that your server really does own that public key, the browser sends its own public key to the server. This is really as far as you need to understand in order to set up your own webserver. Once the server and browser have exchanged public keys, the rest is automatic. For the sake of completion in this blog post, you should know that they don't continue using this asymmetric encryption for long. This secure channel is only used to set up a symmetric encryption method - that is, an encryption method where both sides have identical keys for encryption and decryption. &nbsp;The actual content is sent over this symmetrically encrypted connection, because symmetrical encryption is lighter on the CPU and the data overhead, and is actually harder to break.</div>


<div>&nbsp;</div>


<div>All we sysadmins really care about is the handshake, because that's where all those confusing key files go. So, TL;DR time. Here's what you need:</div>


<div>&nbsp;</div>


<div>1) Your server's private key file. Note that this can be encrypted with a password, but most web hosts would rather you not. If there's a password, someone has to enter it every time they restart the server!</div>


<div>2) The SSL Certificate, signed by your Certificate Authority.</div>


<div>3) Your Certificate Authority's chain of intermediate certificates.</div>


<div>&nbsp;</div>


<div>Most (Drupal-centric) web hosts will want all of these in PEM format. PEM is a text format, which means you can just copy and paste the key around. If your certificates are in another format, it's pretty easy to convert them to PEM with the openssl command, like this:</div>


<div>&nbsp;</div>


<div>
<pre class="brush: bash; auto-links: true; collapse: false; first-line: 1; html-script: false; smart-tabs: true; tab-size: 4; toolbar: true; codetag"> openssl &lt;old-cert-format&gt; -in old-cert-file -out new-cert-file.pem </pre>
</div>


<div>There are also a handful of options that can come in handy in specifying the output, but they really go beyond the scope of this blog post. The resource I always use is the <a href="https://twiki.cern.ch/twiki/bin/view/LinuxSupport/OpenSSLCheatsheet">CERN openSSL conversions cheat sheet</a>.&nbsp;</div>

]]></content>
  </entry>
  
</feed>
