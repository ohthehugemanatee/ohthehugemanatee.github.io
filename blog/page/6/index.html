
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Oh The Huge Manatee</title>
  <meta name="author" content="Campbell Vertesi (ohthehugemanatee)">

  
  <meta name="description" content="Ouch, Sony. Hot on the heels of the last 4 hacks have come another two in a row. Sony BMG Greece was hacked on Monday, and now Sony BMG Japan.Though &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://ohthehugemanatee.github.io/blog/page/6">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Oh The Huge Manatee" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-46172182-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Oh The Huge Manatee</a></h1>
  
    <h2>Drupal, Sysadminning, and Tech.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.duckduckgo.com/" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:ohthehugemanatee.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/05/ouch-sony.html">Sony Hacked AGAIN - Time to Turn Out the Lights?</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2011-05-24T23:37:00+02:00" pubdate data-updated="true">May 24<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content">
<div class="css-full-post-content js-full-post-content">
Ouch, Sony.  Hot on the heels of the last 4 hacks have come another two in a row.  <a href="http://technolog.msnbc.msn.com/_news/2011/05/23/6699560-greek-sony-music-site-hacked-user-data-exposed">Sony BMG Greece was hacked</a> on Monday, and now <a href="http://www.thehackernews.com/2011/05/lulzsec-leak-sonys-japanese-websites.html">Sony BMG Japan</a>.<br /><br />Though I stand by my comments from my earlier post (see: <a href="http://swearingatcomputers.blogspot.com/2011/05/what-sony-hacks-tell-us-about-their.html">What the Sony hacks tell us about their Sysadmins and Management</a>), there is another important dimension to this story.  That&#8217;s the dimension of a feeding frenzy.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-qJsPK31FLKw/Tdw9PebLojI/AAAAAAAAAAk/rEfQrjhsVeA/s1600/Feeding_Frenzy_Gray_Reef_Sharks_Bahamas_1440x1080.jpeg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="240" src="http://3.bp.blogspot.com/-qJsPK31FLKw/Tdw9PebLojI/AAAAAAAAAAk/rEfQrjhsVeA/s320/Feeding_Frenzy_Gray_Reef_Sharks_Bahamas_1440x1080.jpeg" width="320" /></a></div><br />It&#8217;s a popular saying that &#8220;dogs can smell fear.&#8221;  There are similar stories about sharks with blood - actually, even chickens respond to blood with a pecking frenzy of their own.  Anecdotally we all know people who react the same way.  Maybe it&#8217;s something in our animal nature, but this is definitely a similar issue.  <br /><br />People love to jump on the big guy when he&#8217;s down.  After the first big attack on Sony, hackers around the world had their interest piqued.  It was a pretty big security lapse, after all!  Then we found out that it was actually TWO hacks.  By the time the third attack hit, there was definitely blood in the water.  <br /><br />No question, there are serious lapses in judgment that have left Sony&#8217;s many arms exposed in so many ways.  But at the same time, we should recognize that very few organizations could withstand a true hacker feeding frenzy like what Sony is going through.  In the network security world, it is commonly acknowledged that a determined enough opponent will always find a way in.  That&#8217;s why we use honeypots, traffic monitors, and intrusion detection systems, after all.  That&#8217;s why it&#8217;s so important to encrypt sensitive data even when it&#8217;s on a privileged server.  And here we have <a href="http://en.wikipedia.org/wiki/Schadenfreude">schadenfreude</a> acting as a focusing lens for the power of thousands of hackers of various levels.  It&#8217;s not a single super-determined hacker - it&#8217;s worse.  It&#8217;s a thousand dilettantes who know just enough to be dangerous. <br /><br />This is difficult for non-technical people to understand, but in my mind I think of it like wrestling.  Andre the Giant could wrestle almost any man alive.  But try and pit him against 50 8 year olds, and see what happens. Likewise, one script kiddie is not a big deal, but a thousand of them is a different question. The worst part is knowing that in amongst the thousands of idle script runners there are probably some decent blackhats working the more complicated angles.  As the security expert trying to coordinate defense, this is a &#8220;worst nightmare&#8221; scenario.<br /><br />Sony faces threats from all comers at the moment, and there really isn&#8217;t a good response.  They are already hemorrhaging money into their security problem, and it&#8217;s only going to get worse.  Maybe the best possible response is just to turn out the lights.<br /><br />If I were Sony, I would seriously consider putting all internet-connected services into non-interactive modes and &#8220;going dark&#8221; for a week.  Lock everything down as much as possible to let the shark pool calm down a bit.  You can still offer your websites as flat HTML, but I would host them externally and give zero access to privileged information.  Meanwhile, I would have my security team work their asses off to get the house in order.  Change every password and private key, and even change your internal network topography just enough to make anyone&#8217;s prior information invalid.  Check every service and every port, and monitor everything you can think of.  It&#8217;s important to make all of these changes in a hermetically sealed, zero-open-services environment.  <br /><br />After the feeding frenzy has had a week of no targets, bring services back up one at a time, under close scrutiny.  In just a week without even a system to work on, the sharks will start to drift away.  Focus the efforts of what remains of the frenzy on one or two carefully guarded systems at a time.  Give yourself the best possible chance to fend off attacks and respond quickly.  Within a month the sharks will have moved on, and you will be back to full service again in a better state than when you started.<br /><br />This is an expensive tactic to take.  But I would at least want to consider the cost-benefit comparison between a degradation of service for 2-3 weeks, and more security breaches.  How much will it cost the company in reputation, market share, and legal fees, to lose another hundred million credit card numbers?  How much will they have to spend to earn back people&#8217;s trust?  And how much investment will it take before &#8220;Sony customer&#8221; no longer means &#8220;identity theft victim&#8221; in the popular consciousness?
</div>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/05/complete-drupal-cache-serving-http-and.html">The Complete Drupal Cache - Serving HTTP and HTTPS Content With Varnish</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2011-05-22T21:07:00+02:00" pubdate data-updated="true">May 22<span>nd</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content">
<div class="css-full-post-content js-full-post-content">
Varnish is a fantastic caching proxy, commonly used for CMSes.  It&#8217;s not uncommon to see benchmarks boasting 300-500 page loads per second - I&#8217;ve seen benches up to 5000 hits per second.  That&#8217;s faster than serving flat HTML from Apache; we&#8217;re talking about a serious benefit to your server load here.<br /><br />Part of Varnish&#8217;s tremendous speed comes from how lean it is.  At only 58,000 lines of code, it&#8217;s very lightweight.  Unfortunately, this necessitates a no-frills approach.  And SSL is a frill.  <br /><br />I think it&#8217;s very well put by Poul Henning-Kamp (lead developer on the Varnish project) in this <a href="http://www.varnish-cache.org/lists/pipermail/varnish-misc/2010-April/004044.html">mailing list post</a>:<br /><br /><blockquote>I have two main reservations about SSL in Varnish:<br /><br />1. OpenSSL is almost 350.000 lines of code, Varnish is only 58.000,<br />   Adding such a massive amount of code to Varnish footprint, should<br />   result in a very tangible benefit.<br /><br />   Compared to running a SSL proxy in front of Varnish, I can see<br />   very, very little benefit from integration.  Yeah, one process<br />   less and only one set of config parameters.<br /><br />   But that all sounds like &#8220;second systems syndrome&#8221; thinking to me,<br />   it does not really sound lige a genuine &#8220;The world would become<br />   a better place&#8221; feature request.<br /><br />   But I do see some some serious drawbacks:  The necessary changes<br />   to Varnish internal logic will almost certainly hurt varnish <br />   performance for the plain HTTP case.  We need to add an inordinate<br />   about of overhead code, to configure and deal with the key/cert<br />   bits.<br /><br />2. I have looked at the OpenSSL source code, I think it is a catastrophe<br />   waiting to happen.  In fact, the only thing that prevents attackers<br />   from exploiting problems more actively, is that the source code is<br />   fundamentally unreadable and impenetrable.<br /><br />Unless those two issues can be addressed, I don&#8217;t see SSL in Varnish<br />any time soon.</blockquote><br />Ouch.  But that doesn&#8217;t help those of us who want Varnish&#8217;s speed with SSL&#8217;s security.  Really the only solution is to set up an SSL proxy in front of Varnish.  There are <em>lots</em> of ways to do this.  I will show you what I think is the easiest option: Pound and Varnish.<br /><br /><h2>1) Set up Varnish</h2><br />I assume that you&#8217;ve already got a running Apache installation going.  So now we have to put Varnish in front of it.  The first step is to get Apache off of port 80 - that&#8217;s where Varnish is going to live.  In order to do this, we have to find the &#8220;Listen&#8221; line in Apache&#8217;s configuration. On a standard install, it reads something like:<br /><br /><blockquote><code>Listen 0.0.0.0:80</code></blockquote><br />You want to change that to another port.  8080 is a popular one, but it can really be anything above 1024.  In Debian systems you can find this line in /etc/apache2/ports.conf .  In CentOS it&#8217;s in /etc/httpd/conf/httpd.conf .  If you&#8217;re not sure where it is, try grepping for the standard help text around it:  <code>grep "Change this to Listen on specific IP addresses" /etc/apache2/* -r</code>.  You also want to make sure it only serves pages to localhost, so outsiders can&#8217;t attack your Apache directly.  Modify the line to look like this:<br /><br /><blockquote><code>Listen 127.0.0.1:8080</code></blockquote><br />Now let&#8217;s install and configure varnish. On Debian/Ubuntu you can install it from apt repositories: <code>apt-get install varnish</code>.  On CentOS, you first have to add the right repository for yum.  You can install the &#8220;Extra Packages for Enterprise Linux&#8221; (EPEL) repo via RPM - get your version-and-architecture-appropriate link from the <a href="http://fedoraproject.org/wiki/EPEL">EPEL site</a>.  I used:<br /><br /><blockquote><code>sudo rpm -Uvh http://fr2.rpmfind.net/linux/epel/5/x86_64/epel-release-5-4.noarch.rpm<br />sudo yum install varnish</code></blockquote><br />Varnish is configured in two places.  General command line options that are passed directly to the daemon are set in /etc/sysconfig/varnish , and specific behaviors for the proxy are configured in a .vcl file stored in /etc/varnish.<br /><br />Varnish is extremely configurable and tune-able, but this guide will focus on the basics you need for Drupal 7 (Drupal 6 only works if you use Pressflow rather than vanilla Drupal, but that&#8217;s well documented elsewhere).  First, edit the daemon options at /etc/sysconfig/varnish .  The default file gives you four alternative configurations to choose from - we want configuration 2, the first one that uses a .vcl . Uncomment the DAEMON_OPTS lines there, and change the &#8220;listen&#8221; port to 80, and name your own .vcl file.  Here&#8217;s my mostly default daemon_opts . <br /><br /><blockquote><code>DAEMON_OPTS="-a :80 \<br />             -T localhost:6082 \<br />             -f /etc/varnish/swearingatcomputers.com.vcl \<br />             -u varnish -g varnish \<br />             -s file,/var/lib/varnish/varnish_storage.bin,1G"<br /></code></blockquote><br />Save the file.  Now we&#8217;ll set up the .vcl file to configure the proxy itself.  This is my .vcl , you can pretty safely just dump it into the .vcl you named in the DAEMON_OPTS above:<br /><br /><blockquote><code><br />backend default {<br />  .host = "127.0.0.1";<br />  .port = "8080";<br />}<br /><br />sub vcl_recv {<br />#  // Remove has_js and Google Analytics __* cookies.<br />  set req.http.Cookie = regsuball(req.http.Cookie, "(^|;\s*)(__[a-z]+|has_js)=[^;]*", "");<br />#  // Remove a ";" prefix, if present.<br />  set req.http.Cookie = regsub(req.http.Cookie, "^;\s*", "");<br />#  // Remove empty cookies.<br />  if (req.http.Cookie ~ "^\s*$") {<br />    unset req.http.Cookie;<br />  }<br /><br />#  // fix compression per http://www.varnish-cache.org/trac/wiki/FAQ/Compression<br />  if (req.http.Accept-Encoding) {<br />    if (req.url ~ "\.(jpg|png|gif|gz|tgz|bz2|tbz|mp3|ogg)$") {<br />        # No point in compressing these<br />        remove req.http.Accept-Encoding;<br />    } elsif (req.http.Accept-Encoding ~ "gzip") {<br />        set req.http.Accept-Encoding = "gzip";<br />    } elsif (req.http.Accept-Encoding ~ "deflate" && req.http.user-agent !~ "MSIE") {<br />        set req.http.Accept-Encoding = "deflate";<br />    } else {<br />        # unkown algorithm<br />        remove req.http.Accept-Encoding;<br />    }<br />  }<br /><br />}<br /><br />sub vcl_hash {<br />  if (req.http.Cookie) {<br />    set req.hash += req.http.Cookie;<br />  }<br />}<br /></code></blockquote><br />The bulk of this file is occupied with making sure that cookies aren&#8217;t cached, and solving a problem with compression.   The only part that you should be concerned with editing is the bit at the top, <code>backend default {</code>. This is where you tell Varnish about all the back ends for which it should cache.  Varnish is a great load balancer, so if you have 5 systems on the back end which are all serving content, you can list them here.  Each one would get it&#8217;s own &#8220;Backend&#8221; declaration.  If you want to load balance, see a different guide.  We&#8217;re just interested in the caching for now.  So set the .host and .port variables to match your setup - very likely you want to keep them the same.<br /><br />Now test the whole thing by restarting apache and starting varnish.<br /><br /><blockquote><code>sudo service httpd restart<br />sudo service varnish restart</code></blockquote><br />If you don&#8217;t see any errors, you&#8217;re good to go!  If you just get a generic [FAILED] for Varnish, without any error messages, there&#8217;s probably a syntax problem with your VCL.  <br /><br /><h2>2) Set up your SSL certs for Pound</h2><br />Create your server&#8217;s private key and certificate request.  I get confused easily between the different certs, so I name them in an idiot proof way that you might want to copy:<br /><br /><blockquote><code>openssl req -new -newkey rsa:2048 -nodes -keyout swearingatcomputers.com.private.key -out swearingatcomputers.com.certreq.pem</code></blockquote><br />Traditionally, your private key should go in /etc/ssl/private on Debian/Ubuntu , or /etc/pki/tls/private on CentOS.  It really doesn&#8217;t matter, but this gives you a nice central place to store your certs. <br /><br />Now use that certificate request to get a signed cert.  I get mine on the cheap from Godaddy ($50/yr is hard to beat!), but if you just want to test, you can make a locally-signed cert like this:<br /><br /><blockquote><code>openssl x509 -req -days 365 -in swearingatcomputers.com.certreq.pem -signkey swearingatcomputers.com.private.key -out swearingatcomputers.com.selfsigned.crt</code></blockquote><br />The signed cert typically goes in /etc/ssl/certs .<br /><br />For a normal SSL setup, this is all you need.  But Pound likes both the certificates in a single file, so we&#8217;re going to have to make a special combined version for pound.  <br /><br /><blockquote><code><br />openssl x509 -in /etc/ssl/certs/swearingatcomputers.com.crt -out /etc/ssl/private/swearingatcomputers.com.combined.pem<br />openssl rsa -in /etc/ssl/private/swearingatcomputers.com.private.key >> /etc/ssl/private/swearingatcomputers.com.combined.pem<br /></code></blockquote><br />Now we&#8217;re ready to set up Pound.<br /><br /><h2>3) Set up the Pound SSL proxy</h2><br />This part surprised me with how easy it is.  Pound is a great system that is very simple to configure!  Install it using apt-get or yum: <code>yum install pound</code>, then configure it at /etc/pound.cfg .<br /><br />First comment out or delete the <code>ListenHTTP</code> section.  We don&#8217;t want Pound to listen on port 80 at all.<br /><br />Then we&#8217;ll set up the <code>ListenHTTPS</code> section.  Apart from telling it to listen on all devices&#8217; port 443 and giving it the cert, we&#8217;re going to make sure it sets a special header to notify Drupal that it&#8217;s been forwarded from an HTTPS proxy.  We&#8217;re also going to make sure that GET and PUT operations are supported.  Then at the end, we will tell it where to find the back end (Varnish, in our case) - port 80.  Here&#8217;s my pound config:<br /><br /><blockquote><code><br />User "pound"<br />Group "pound"<br />Control "/var/lib/pound/pound.cfg"<br /><br />#ListenHTTP<br />#    Address 0.0.0.0<br />#    Port 80<br />#End<br /><br />ListenHTTPS<br />    Address 0.0.0.0<br />    Port    443<br />    Cert    "/etc/ssl/certs/swearingatcomputers.com.crt.pem"<br /><br />    # set X-Forwarded-Proto so D7 knows we're behind an HTTPS proxy.<br />    HeadRemove "X-Forwarded-Proto"<br />    AddHeader "X-Forwarded-Proto:https"<br /><br />    #Allow PUT and DELETE too<br />    xHTTP       0<br />End<br /><br />Service<br />    BackEnd<br />        Address 127.0.0.1<br />        Port    80<br />    End<br />End<br /></code></blockquote><br />Save the config file, and start pound with <code>service pound start</code>.  There you go, you&#8217;ve got an HTTPS forwarder.  <br /><br /><h2>4) Make Drupal HTTPS aware</h2><br />One big problem with the setup so far, is that Drupal doesn&#8217;t know that it&#8217;s serving HTTPS content.  Remember, as far as Apache is concerned, it&#8217;s just HTTP served in the clear to Varnish.  Even Varnish doesn&#8217;t really know about the HTTPS on the front end.  We&#8217;re going to follow this X-Forwarded-Proto:https header back through the stack to make sure that every level interprets it properly.<br /><br />First we deal with Varnish.  Let&#8217;s make sure that the X-forwarded-proto header is delivered to Apache intact.  Find the <code>sub vcl_hash</code> section of your .vcl file, <code>/etc/varnish/swearingatcomputers.com.vcf</code>, and add these lines:<br /><br /><blockquote><code><br />if (req.http.x-forwarded-proto) {<br />set req.hash += req.http.x-forwarded-proto;<br />}<br /></code></blockquote><br />If you&#8217;re using my template above, the whole section will look like this:<br /><br /><blockquote><code><br />sub vcl_hash {<br />  if (req.http.Cookie) {<br />    set req.hash += req.http.Cookie;<br />  }<br />  if (req.http.x-forwarded-proto) {<br />        set req.hash += req.http.x-forwarded-proto;<br />  }<br />}<br /></code></blockquote><br />You&#8217;ll have to restart Varnish after making this change.<br /><br />Now let&#8217;s make sure that Drupal knows to look for this header.  D7 has some variables for this in it&#8217;s settings.php , just waiting to be uncommented.  You can walk through the explanations in the file itself and uncomment the relevant lines, or just add this at the end:<br /><br /><blockquote><code> <br /># Settings for Varnish - tell Drupal that it's behind a reverse proxy<br /><br />$conf['reverse_proxy'] = TRUE;<br />$conf['reverse_proxy_addresses'] = array('127.0.0.1');<br /><br />$conf['page_cache_invoke_hooks'] = FALSE;<br /><br /># Settings for HTTPS cache - tell Drupal that forwarded https is the real thing<br />if (isset($_SERVER['HTTP_X_FORWARDED_PROTO']) &&<br />  $_SERVER['HTTP_X_FORWARDED_PROTO'] == 'https') {<br />  $_SERVER['HTTPS'] = 'on';<br />}<br /></code></blockquote><br /><h2>5) Test and brag</h2><br />That&#8217;s it - you should have your proxy configured!  You can do a simple test to make sure it&#8217;s working by watching varnishlog for cache hits. Simply <code>varnishlog |grep hit</code> in your terminal, and try refreshing the frontpage of your site.  You should see a few lines of hits pop up in the log.  (If not, you might want to try grepping for &#8220;pass&#8221; or &#8220;miss&#8221; to help work out what&#8217;s happening)  <br /><br />Now let&#8217;s see how this caching holds up under load.  After all, that&#8217;s the whole point, right?  I like a simple ab test<br /><br /><blockquote>ab -c 40 -n 5000 -q http://swearingatcomputers.com/</code></blockquote><br />This will simulate 5000 hits on the frontpage, at a rate of 40 per second.  Look for &#8220;Requests per second&#8221;, that&#8217;s my favorite statistic here. On my &#8220;playing around&#8221; Amazon Micro instance, I pull about 650 hits per second.  In theory, this smallest of VPS servers could handle over 2 million hits per hour!  <br /><br />I love Varnish.
</div>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/05/what-sony-hacks-tell-us-about-their.html">What the Sony Hacks Tell Us About Their Sysadmins and Management</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2011-05-21T00:04:00+02:00" pubdate data-updated="true">May 21<span>st</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content">
<div class="css-full-post-content js-full-post-content">
I wouldn&#8217;t want to be a Sony sysadmin right now. <a href="http://www.f-secure.com/weblog/archives/00002160.html">F-Secure just blogged</a> about evidence of a <strong>fourth</strong> hack at the electronics giant.   This one is relatively harmless - a phishing scam being run from Sony servers - but this pattern of security problems should tell us a few things about Sony&#8217;s SysAdmin staff:<br /><br />1) They&#8217;re powerless in their own organization.<br />2) They don&#8217;t get to set IT resource policy.<br />3) They&#8217;re furious at their bosses right now.<br /><br />It&#8217;s hard to generalize like this across an entire organization, but I&#8217;m willing to bet that in the affected departments this is the case.  Let&#8217;s look at what we know about the hacks.  <br /><br />The first two involved stealing millions of users&#8217; credit card information, which was stored unencrypted and vulnerable during a DoS.  The hackers got in with a list of valid usernames and a dictionary attack.  I don&#8217;t know any SysAdmin, no matter how junior, who would leave 100 million users&#8217; financial and personal information in an unencrypted format.  And on a server with access to this unencrypted financial information, who doesn&#8217;t impose password standards?  I can understand a vulnerability during an exceptional circumstance - a really good SysAdmin team has contingency plans for this sort of thing, but at least it&#8217;s an understandable problem - but dammit, that data was just left out in the open, with only a dictionary password to protect it!  The only thing the DoS did was cover for the dictionary attempts. What SysAdmin team <i>does</i> that?<br /><br />The next &#8220;attack&#8221; was really just a vulnerability in the password reset system.  If you knew a user&#8217;s date of birth and email address, you could gain access to their account.  Again, who actually deploys a system that vulnerable on an enterprise level?  Again, this is an account that includes personal, financial data.  Not even an email confirmation, or personal security question in sight!  Remember this is a service that has a purpose built platform associated with it.  They could ask for the serial number off the back of your PS3, or send you a code to your Playstation Mobile.  They could set up an RSA-style key generator on your device, the way Google does with your mobile phone.  But no - your financial data was protected by information that you publish on Facebook.  What technical lead <i>does</i> that?<br /><br />And finally, this most recent attack is a more conventional hack on a (relatively) unimportant server. It&#8217;s true that by now, Sony&#8217;s many arms must be the target of every wannabe hacker with a cablemodem.  But seriously, don&#8217;t these guys run updates?  Do they have a vulnerability scanner?  Nessus should give them a license as a charity case.<br /><br />I think the most damning part is the response to the hacks.  In a gesture to try and restore their reputation on security, Sony laid out their technical response:<br /><br /><ul><li>Added automated software monitoring and configuration management to help defend against new attacks</li><li>Enhanced levels of data protection and encryption</li><li>Enhanced ability to detect software intrusions within the network, unauthorized access and unusual activity patterns</li><li>Implementation of additional firewalls</li></ul><br />Wait a moment - are they telling us they DIDN&#8217;T have these things before?  SONY, of all companies, didn&#8217;t have an intrusion detection system?  They didn&#8217;t encrypt their data?  This megagiant of digital services had no internal firewalls, no security scans?  <br /><br /><h2>Don&#8217;t blame the SysAdmins</h2><br />It seems like I&#8217;m going to come down on the SysAdmin team over there.  I&#8217;m not.  I know what it&#8217;s like working in a business environment.  Most importantly, I know what it&#8217;s like working in an environment where a <i>manager</i>, not a technical person, is calling the shots.  And to me, this reeks of it.  <br /><br />This smacks of a work environment where the technical people are told not to &#8220;waste&#8221; time on things like &#8220;updates&#8221;, or to make things too complicated with &#8220;security&#8221;.  A workplace where the SysAdmins have been saying for months that this stuff is important, but management has set other priorities.  <br /><br />It&#8217;s frustrating being in that kind of environment on a day to day basis, but people get by.  There&#8217;s plenty of new work to get to, plenty of issues that your boss thinks are more profitable than upkeep.  And the admins can&#8217;t understand why the bosses don&#8217;t want to spend time on these critical, invisible tasks.  It seems like such a simple principle, and there are a million metaphors for it: feeding the golden goose, getting your teeth checked, getting an oil change&#8230; whatever you want to call it, management is not interested.  And when the irresponsibility of management priorities finally comes home to roost, it&#8217;s the SysAdmins who have to stay weekends and nights to fix it.  It&#8217;s the SysAdmins who have to explain to everyone that data was unencrypted, that security patches weren&#8217;t applied, that pentesting was never considered.  <br /><br />I&#8217;ve seen that happen at too many organizations to count, and I predict that a lot of Sony SysAdmins are secretly trying to find other jobs right now.  Looking for a new SysAdmin?  Pick up one of the guys at Sony!  They&#8217;d be happy for a working environment that lets them do their job.
</div>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/05/sendmail-return-path-php-and-importance.html">Sendmail Return-path, Php, and the Importance of the Variables You Don&#8217;t Think Of</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2011-05-20T20:37:00+02:00" pubdate data-updated="true">May 20<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content">
<div class="css-full-post-content js-full-post-content">
I got called in for an interesting issue on one of the servers I co-maintain today. &nbsp;Sites were messing up left right and center, in all sorts of bizarre ways. &nbsp;CSS wouldn&#8217;t be delivered, or half of it would be, or some of the content would disappear&#8230; and different on every page load! &nbsp;It only took me a moment to confirm - the server was out of space on the root device.<br /><br />This was readily visible in the logs - any mysql transaction you tried would return &#8220;Got error 28 from storage engine&#8221;, which typically means the drive is out of space. &nbsp;And a quick <code>df -ht</code> confirmed:<br /><br /><blockquote><code><br />root@production:/# df -h<br />Filesystem &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Size &nbsp;Used Avail Use% Mounted on<br />/dev/sda1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 9.9G &nbsp;9.4G &nbsp;160K 100% /<br />none &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7.3G &nbsp;128K &nbsp;7.3G &nbsp; 1% /dev<br />none &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7.6G &nbsp; &nbsp; 0 &nbsp;7.6G &nbsp; 0% /dev/shm<br />none &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7.6G &nbsp; 76K &nbsp;7.6G &nbsp; 1% /var/run<br />none &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7.6G &nbsp; &nbsp; 0 &nbsp;7.6G &nbsp; 0% /var/lock<br />none &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;7.6G &nbsp; &nbsp; 0 &nbsp;7.6G &nbsp; 0% /lib/init/rw<br />/dev/sdb &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;414G &nbsp;5.9G &nbsp;387G &nbsp; 2% /mnt<br />/dev/sdf &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 50G &nbsp;8.8G &nbsp; 42G &nbsp;18% /ebs</code></blockquote><div><br /></div><div>Yep, /dev/sda1 was at 100% . &nbsp;So I ran through the usual suspects, and tracked it down to /var/log pretty quickly:</div><div><br /></div><blockquote><code><br /><div>root@production:/var# du -h |grep G</div><div>7.2G ./log</div><div>7.8G . </div></code></blockquote><div><br /></div><div>Sweet noodly goodness, 7 gigs of logs? &nbsp;</div><div><br /></div><div><blockquote><code><div>root@production:/var/log# ls -lh |grep G</div><div>total 6.6G</div><div>-rw-r----- 1 syslog &nbsp; &nbsp;adm &nbsp;2.2G 2011-05-20 18:51 mail.info</div><div>-rw-r----- 1 syslog &nbsp; &nbsp;adm &nbsp;2.2G 2011-05-20 18:51 mail.log</div></code></blockquote><div><br /></div></div><div>Specifically, the mail logs were taking up all the space. &nbsp;The first order of business was to get them off the root device, so I moved them to /mnt , the freebie extra-ethereal EBS volume that Amazon gives you with every S3 instance. &nbsp;Then I could restart mysql/apache2, and at least the sites would be back up and normal. So what was going on in these logs?</div><br /><br /><br /><blockquote><code><br />root@production:/var/log# tail /mnt/mail.log <br /><br />May 20 18:51:56 ip-10-118-81-22 sm-mta[21749]: p4JGv30H001061: to=&lt;www-data@production.swearing.com&gt;, delay=1+01:54:53, xdelay=00:00:00, mailer=esmtp, pri=13980000, relay=production.swearing.com., dsn=4.0.0, stat=Deferred: Connection timed out with production.swearing.com. <br /><br />May 20 18:51:56 ip-10-118-81-22 sm-mta[21749]: p4JGuuWt000738: to=&lt;www-data@production.swearing.com&gt;, delay=1+01:55:00, xdelay=00:00:00, mailer=esmtp, pri=13980000, relay=production.swearing.com., dsn=4.0.0, stat=Deferred: Connection timed out with production.swearing.com. <br /><br />May 20 18:51:56 ip-10-118-81-22 sm-mta[21749]: p4JGvCZe001480: to=&lt;www-data@production.swearing.com&gt;, delay=1+01:54:44, xdelay=00:00:00, mailer=esmtp, pri=13980000, relay=production.swearing.com., dsn=4.0.0, stat=Deferred: Connection timed out with production.swearing.com. <br /><br />May 20 18:51:56 ip-10-118-81-22 sm-mta[21749]: p4JGuf9L032603: to=&lt;www-data@production.swearing.com&gt;, delay=1+01:55:15, xdelay=00:00:00, mailer=esmtp, pri=13980000, relay=production.swearing.com., dsn=4.0.0, stat=Deferred: Connection timed out with production.swearing.com. <br /><br />May 20 18:51:56 ip-10-118-81-22 sm-mta[21749]: p4JGv5qh001300: to=&lt;www-data@production.swearing.com&gt;, delay=1+01:54:51, xdelay=00:00:00, mailer=esmtp, pri=13980000, relay=production.swearing.com., dsn=4.0.0, stat=Deferred: Connection timed out with production.swearing.com. <br /><br />May 20 18:51:56 ip-10-118-81-22 sm-mta[21749]: p4JGvPKQ002038: to=&lt;www-data@production.swearing.com&gt;, delay=1+01:54:31, xdelay=00:00:00, mailer=esmtp, pri=13980000, relay=production.swearing.com., dsn=4.0.0, stat=Deferred: Connection timed out with production.swearing.com. <br /><br />May 20 18:51:56 ip-10-118-81-22 sm-mta[21749]: p4JGvHfI001678: to=&lt;www-data@production.swearing.com&gt;, delay=1+01:54:39, xdelay=00:00:00, mailer=esmtp, pri=13980000, relay=production.swearing.com., dsn=4.0.0, stat=Deferred: Connection timed out with production.swearing.com. <br /><br />May 20 18:51:56 ip-10-118-81-22 sm-mta[21749]: p4JGvMxj001908: to=&lt;www-data@production.swearing.com&gt;, delay=1+01:54:34, xdelay=00:00:00, mailer=esmtp, pri=13980000, relay=production.swearing.com., dsn=4.0.0, stat=Deferred: Connection timed out with production.swearing.com. <br /><br />May 20 18:51:56 ip-10-118-81-22 sm-mta[21749]: p4JGvHgB001693: to=&lt;www-data@production.swearing.com&gt;, delay=1+01:54:3May 20 18:53:11 ip-10-118-81-22 sm-mta[21749]: p4FJ5XYp028180: to=&lt;www-data@production.swearing.com&gt;, delay=4+23:40:11, xdelay=00:00:00, mailer=esmtp, pri=64650000, relay=production.swearing.com., dsn=4.0.0, stat=Deferred: Connection timed out with production.swearing.com.<br /><div><br /></div></code></blockquote><div>OK, clearly something is looping delivery failures. &nbsp;This server hosts several websites, without their accompanying email, so it&#8217;s configured to force an MX lookup before attempting to send anything anywhere. &nbsp;But it doesn&#8217;t receive mail, so it would have just timed out waiting for itself to respond.&nbsp;</div><div><br /></div><div>This is a problem, but that&#8217;s a condition that should never happen. &nbsp;So what on earth would make it try to &nbsp;deliver mail to production.swearing.com? &nbsp;I headed over to the mail queue to see. &nbsp;The output of mailq was enormous, and I had to Ctrl+C it partway through. &nbsp;Every record was &#8220;Deferred&#8221; due to a timeout on production.swearing.com . &nbsp; OK, so clearly there&#8217;s a lot in the queue. &nbsp;So I had a look at some of the messages.</div><div><br /></div><blockquote><code><br /><div><div>root@production:/var/spool/mqueue# cat dfp4I0uHUF025487&nbsp;</div><div><br /></div><div>This is a MIME-encapsulated message</div><div><br /></div><div>--p4I0uHUF025487.1305680177/ip-10-118-81-22.ec2.internal</div><div><br /></div><div>The original message was received at Wed, 18 May 2011 00:56:14 GMT</div><div>from localhost [127.0.0.1]</div><div><br /></div><div>&nbsp; &nbsp;----- The following addresses had permanent fatal errors -----</div><div>&lt;asf@gmail.com&gt;</div><div>&nbsp; &nbsp; (reason: 550-5.1.1 The email account that you tried to reach does not exist. Please try)</div><div><br /></div><div>&nbsp; &nbsp;----- Transcript of session follows -----</div><div>... while talking to gmail-smtp-in.l.google.com.:</div><div>&gt;&gt;&gt; DATA</div><div>&lt;&lt;&lt; 550-5.1.1 The email account that you tried to reach does not exist. Please try</div><div>&lt;&lt;&lt; 550-5.1.1 double-checking the recipient's email address for typos or</div><div>&lt;&lt;&lt; 550-5.1.1 unnecessary spaces. Learn more at &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</div><div>&lt;&lt;&lt; 550 5.1.1 http://mail.google.com/support/bin/answer.py?answer=6596 dz10si728275vdc.78</div><div>550 5.1.1 &lt;asf@gmail.com&gt;... User unknown</div><div>&lt;&lt;&lt; 503 5.5.1 RCPT first. dz10si728275vdc.78</div><div><br /></div><div>--p4I0uHUF025487.1305680177/ip-10-118-81-22.ec2.internal</div><div>Content-Type: message/delivery-status</div><div><br /></div><div>Reporting-MTA: dns; ip-10-118-81-22.ec2.internal</div><div>Received-From-MTA: DNS; localhost</div><div>Arrival-Date: Wed, 18 May 2011 00:56:14 GMT</div><div><br /></div><div>Final-Recipient: RFC822; asf@gmail.com</div><div>Action: failed</div><div>Status: 5.1.1</div><div>Remote-MTA: DNS; gmail-smtp-in.l.google.com</div><div>Diagnostic-Code: SMTP; 550-5.1.1 The email account that you tried to reach does not exist. Please try</div><div>Last-Attempt-Date: Wed, 18 May 2011 00:56:17 GMT</div><div><br /></div><div>--p4I0uHUF025487.1305680177/ip-10-118-81-22.ec2.internal</div><div>Content-Type: text/rfc822-headers</div><div><br /></div><div>Return-Path: &lt;www-data@production.swearing.com&gt;</div><div>Received: from production.swearing.com (localhost [127.0.0.1])</div><div><span class="Apple-tab-span" style="white-space: pre;"> </span>by ip-10-118-81-22.ec2.internal (8.14.3/8.14.3/Debian-9.1ubuntu1) with ESMTP id p4I0uEUF025482</div><div><span class="Apple-tab-span" style="white-space: pre;"> </span>for &lt;asf@gmail.com&gt;; Wed, 18 May 2011 00:56:14 GMT</div><div>Received: (from www-data@localhost)</div><div><span class="Apple-tab-span" style="white-space: pre;"> </span>by production.swearing.com (8.14.3/8.14.3/Submit) id p4I0uEcp025480;</div><div><span class="Apple-tab-span" style="white-space: pre;"> </span>Wed, 18 May 2011 00:56:14 GMT</div><div>Date: Wed, 18 May 2011 00:56:14 GMT</div><div>Message-Id: &lt;201105180056.p4I0uEcp025480@production.swearing.com&gt;</div><div>To: asf@gmail.com</div><div>Subject: Swearing at Computers :: new comment for your post.</div><div>MIME-Version: 1.0</div><div>Content-Type: text/plain; charset=UTF-8; format=flowed; delsp=yes</div><div>Content-Transfer-Encoding: 8Bit</div><div>X-Mailer: Drupal</div><div>Errors-To: info@swearing.com</div><div>Sender: info@swearing.com</div><div>From: info@swearing.com</div><div><br /></div><div>--p4I0uHUF025487.1305680177/ip-10-118-81-22.ec2.internal--</div></div></code></blockquote><div><br /></div><div><br /></div><div>They were all in this vein. &nbsp;So for some reason, the swearing.com site was trying to send out a comment notification to a non-existent email address, which was getting rejected (rightly so). &nbsp;A little inspection in Drupal bore this out: asf@gmail.com, and many other addresses from these queued messages, were being subscribed to comment notifications. &nbsp;There is a comment form somewhere on swearing.com that allows people to sign up for notifications without going through a CAPTCHA. &nbsp;I took this up with the owner of the site immediately.</div><div><br /></div><div>But there&#8217;s still another layer of problem here. &nbsp;Why were delivery failures not being sent to the from: address? &nbsp;A little research led me to the appropriate <a href="http://www.faqs.org/rfcs/rfc2821.html">RFP</a>. &nbsp;The Return-path header is added by the MTA at the last possible moment, specifically as an address for delivery errors. &nbsp;It&#8217;s also used as an indicator when the message is being sent from this host or just relayed. &nbsp;And by default, sendmail uses the return-path of user@hostname. &nbsp;</div><div><br /></div><div>The solution is to have a real email address set up somewhere for this kind of error. &nbsp;You can modify your php.ini (or apache config) to set the default return-path pretty easily. &nbsp;Simply add this to your php.ini :</div><br /><blockquote><code>sendmail_path "/usr/sbin/sendmail -t -i -f servers@trellon.com"</code><br /></blockquote>That&#8217;s it!  Now hopefully I got all the spammy addresses out of that contact form&#8230; or servers@swearing.com is gonna have a lot of mail waiting on Monday!<div><div id="wiki_add_attachment" style="color: #484848; font-family: Verdana, sans-serif; font-size: 12px;"></div></div>
</div>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/05/running-redmine-in-subdirectory.html">Running Redmine in a Subdirectory</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2011-05-16T13:17:00+02:00" pubdate data-updated="true">May 16<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content">
<div class="css-full-post-content js-full-post-content">
<a href="http://www.redmine.org/">Redmine</a> is a fantastic project management tool for technically oriented organizations. &nbsp;At my job, we use it to manage 30-50 active projects at a time, as well as the back history of several hundred projects. &nbsp;Oddly enough, the <a href="http://www.redmine.org/projects/redmine/wiki/RedmineInstall">normal installation process</a> puts it on port 3000 . &nbsp;We had some trouble with developers&#8217; ISPs not allowing HTTPS traffic over non-standard ports, so we had to move redmine onto port 443 along with the rest of the SSL traffic. &nbsp;We use the same hostname for several internal services, and differentiate by subdirectory - so I thought I&#8217;d just put redmine into another subdirectory.<br /><br />This was more painful than I expected it to be.  According to examples in the Redmine forums and many referring blog posts, you should be able to do this with a simple vhost configuration:<br /><blockquote><code>&lt;VirtualHost *:80&gt;<br />&nbsp; ServerName swearing.com<br />&nbsp; DocumentRoot /var/www/html <br /><br />&nbsp; PassengerAppRoot /var/www/html/redmine<br />&nbsp; RailsBaseURI /redmine<br />&nbsp; Alias /redmine /var/www/html/redmine/public<br />&lt;/VirtualHost&gt;</code></blockquote>I&#8217;m leaving out the SSL stuff, because that&#8217;s irrelevant here.  The objective is to run redmine in a subdirectory, on no matter what port. :)<br /><br />From my reading of this configuration, we&#8217;re telling Passenger that it&#8217;s application starts at /var/www/html/redmine , Rails that it should only manage URLs below /redmine , and Apache that /redmine is an alias for the public directory inside Redmine. &nbsp;To me, this makes sense.<br /><br />But there&#8217;s something a little funny about PassengerAppRoot and RailsBaseURI , because Redmine tried to serve EVERYTHING on the vhost&#8230; and of course it would get 404s for all of our other applications in subdirectories. &nbsp;For example, http://swearingatcomputers.blogspot.com/application would get a 404, because Rails isn&#8217;t aware of /application. <br /><br />So maybe this configuration works when you use Passenger for all your projects, but in a mixed environment it is not enough.<br /><br />In the end, here&#8217;s what worked:<br /><blockquote><code> &lt;VirtualHost *:80&gt; <br />&nbsp; ServerName swearing.com</code><br /><div><code>&nbsp; DocumentRoot /var/www/html&nbsp;</code></div><div><code>&nbsp; Alias /redmine /var/www/html/redmine/public</code><br /><code><br /></code><br /><code>&nbsp; &lt;Directory /&gt;</code></div><div><code>&nbsp; Options FollowSymLinks&nbsp;</code></div><div><code>&nbsp; AllowOverride None&nbsp;</code></div><div><code>&nbsp; &lt;/Directory&gt;</code><br /><code><br /></code><br /><code>&nbsp; &lt;Directory /var/www/html/redmine&gt;</code></div><div><code>&nbsp; &nbsp; Options Indexes FollowSymLinks MultiViews&nbsp;</code></div><div><code>&nbsp; &nbsp; AllowOverride All&nbsp;</code></div><div><code>&nbsp; &nbsp; Order allow,deny&nbsp;</code></div><div><code>&nbsp; &nbsp; allow from all&nbsp;</code></div><div><code>&nbsp; &lt;/Directory&gt; </code><br /><code><br /></code><br /><code>&nbsp; &lt;Directory /var/www/html/redmine/public&gt;</code></div><div><code>&nbsp; &nbsp; PassengerEnabled on&nbsp;</code></div><div><code>&nbsp; &nbsp; SetHandler none&nbsp;</code></div><div><code>&nbsp; &nbsp; PassengerAppRoot /var/www/html/redmine</code></div><div><code>&nbsp; &nbsp; RailsBaseURI /redmine </code><br /><code><br /></code><br /><code>&nbsp; &nbsp; Options Indexes FollowSymLinks MultiViews&nbsp;</code></div><div><code>&nbsp; &nbsp; AllowOverride None</code></div><div><code>&nbsp; &nbsp; Order allow,deny</code></div><br /><div><code>&nbsp; &nbsp; allow from all&nbsp;</code><br /><div><code>&nbsp; &lt;/Directory&gt;&nbsp;</code><br /><div><code><br />&lt;/VirtualHost&gt;<br /></code></div></div></div></blockquote>OK, I admit it - we cheated.  Instead of using Passenger and Rails&#8217; options to set their base directory, we used Apache to do it.  Sometimes the simplest solution is the best!
</div>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/05/setting-up-secure-ldap-on-ubuntu-1004.html">Setting Up Secure LDAP on Ubuntu 10.04 - the Idiot&#8217;s Guide to SSL, TLS, and SLAPD</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2011-05-13T21:58:00+02:00" pubdate data-updated="true">May 13<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content">
<div class="css-full-post-content js-full-post-content">
LDAP is one of the Elder Gods of the UNIX world. &nbsp;It had it&#8217;s beginnings in DAP, the first Directory Access Protocol, in 1988. &nbsp;Within a decade DAP was more or less discarded in favor of the Lightweight Directory Access Protocol. &nbsp;And it has stayed in basically the same format ever since.<br /><br />LDAP is, as you might have guessed, a directory protocol. &nbsp;It was developed in response to X.500, a spec for telephone directories&#8230; but personally, I have a hard time thinking about LDAP structure in terms of a phone book. &nbsp;The structure is really a lot more like directories on a drive. &nbsp;The idea is to store and retrieve information in a hierarchical structure. &nbsp;You can technically put anything in there, but most often it is used for things like phone books, personnel directories, and the like. &nbsp;For SysAdmins, it&#8217;s typically used as a Single Sign On method - providing a unified set of usernames, passwords, and basic account information that can be shared across a number of services. &nbsp;I&#8217;ve got it running my user accounts on 4 separate servers, plus about 30 different websites in development and 3 web applications that we use internally. &nbsp;Just thinking about all those &#8220;useradd&#8221; commands I&#8217;m saving makes me misty-eyed.<br /><br /><br /><b>Making sense of LDAP</b><br /><br />Because LDAP is old, it&#8217;s not exactly&#8230; user friendly. &nbsp;There are some terms you need to learn. &nbsp;Chiefly, the terms and acronyms you will see in the &#8220;directory structure.&#8221; &nbsp;Anything in LDAP has a &#8220;Distinguished Name&#8221;. &nbsp;This is just like saying &#8220;full path from root&#8221; in a file directory. &nbsp;The DN tells you what the object is called, and where it sits in the directory hierarchy. &nbsp;If my user account is me@swearingatcomputers.blogspot.com , this is how LDAP would think of it:<br /><blockquote><code>cn=me,dc=swearingatcomputers,dc=blogspot,dc=com</code></blockquote>CN stands for &#8220;Common Name&#8221; - that&#8217;s just the name of the object. &nbsp;Like a filename, it doesn&#8217;t have to be unique, but it does have to be unique inside it&#8217;s directory. &nbsp;You can&#8217;t have two files called /etc/passwd, after all! &nbsp;DC just stands for Domain Component - that means it&#8217;s part of the directory where the CN &#8220;lives&#8221;. <br /><br />Let&#8217;s take another example - you have a file in your home directory called &#8220;swearing&#8221;. &nbsp;In the filesystem, we&#8217;d refer to this as /home/you/swearing.txt . &nbsp;If it were in LDAP, it would be:<br /><blockquote><code>cn=swearing,dc=you,dc=home</code></blockquote>You also have the option of using Organizational Units (OU), which are just like groups. <br /><br />The last thing you need to know is that this system typically mirrors the Domain Name System, at least at the top level. &nbsp;So when we set up your LDAP server (it&#8217;s coming, I promise!), one of the first things you&#8217;ll do is define the &#8220;baseDN&#8221;, or the &#8220;root&#8221; of your directory. &nbsp;If I were hosting LDAP here, it would be dc=swearingatcomputers,dc=blogspot,dc=com .<br /><br />Make sense? &nbsp;You are going to organize all your users this way, so it&#8217;d better make sense!<br /><br /><b>SETTING UP LDAP ON UBUNTU</b><br /><br />The trouble with LDAP is, it seems to be the only thing that is a bitch to set up on Ubuntu. &nbsp;That&#8217;s right, Ubuntu, the distro for everyone. &nbsp;Learning how to set up a secure LDAP installation is cryptic, poorly documented, and just darn finicky. &nbsp;So here&#8217;s my &#8220;dummies&#8221; guide:<br /><br /><b>1) Get your materials</b><br /><blockquote><code>apt-get install slapd ldap-utils</code></blockquote>Slapd is the ldap daemon, and ldap-utils are the utilities you use to work with LDAP. &nbsp;Duh. &nbsp;That was the easy part.<br /><br /><b>2) Set up your schema(s)</b><br /><b><br /></b><br />LDAP is EXTREMELY configurable. &nbsp;You can set up whatever fields you like - it&#8217;s a lot like setting up your own database, in that respect. &nbsp;Except that this protocol was designed by the people who make <i>phone books</i>, so they went ahead and wrote RFCs for the structures you&#8217;re likely to use. &nbsp;There&#8217;s nothing stopping you from making your own schema, but I don&#8217;t recommend you bother. &nbsp;The RFCs are very thoroughly thought out, and dammit the schemas come with the package.<br /><br />We&#8217;re going to load schemas for a company directory which also serves as a Single Sign On system. &nbsp;Each schema is written out like a script, and stored in it&#8217;s own little .ldif file in /etc/ldap/schema . &nbsp;You load them with ldapadd.<br /><blockquote><code> sudo ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/cosine.ldif<br />sudo ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/nis.ldif<br />sudo ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/inetorgperson.ldif</code></blockquote>The first schema is COSINE (<a href="http://www.rfc-editor.org/rfc/rfc4524.txt">RFC4524</a>). &nbsp;This defines the basic fields, like name. &nbsp;It&#8217;s also got a bunch of other ones in there, but trust me - just leave it as it is. &nbsp;Some of those fields are used by other schemas, and if you remember RPM hell, just imagine what it&#8217;s like to be in LDAP field hell.<br /><br />The second is NIS (<a href="http://www.rfc-editor.org/rfc/rfc2307.txt">RFC 2307</a>). &nbsp;You guessed it - those are fields common for Network Information Services. &nbsp;That&#8217;s what you&#8217;re making, so you gotta include this one.<br /><br />The last is Inetorgperson (<a href="http://www.rfc-editor.org/rfc/rfc2798.txt">RFC 2798</a>). &nbsp;This schema is for organizations that use the Internet. &nbsp;Since you&#8217;re reading this, I&#8217;m gonna guess that&#8217;s you. Seriously, if you want to store email addresses, this is you.<br /><br /><b>3) Configure the backend</b><br /><div><br /></div>SLAPD has a pretty sweet feature which lets you store configuration information in the LDAP directory itself. &nbsp;Not only do you get to feel cool for configuring in something other than a flat text file, but it means you can modify the daemon&#8217;s configuration without restarting it. So here&#8217;s an .ldif file that you can tweak and load to set this service up for yourself. <br /><div><blockquote><code><br /></code><br /><div><code># Load dynamic backend module</code></div><div><code>dn: cn=module,cn=config</code></div><div><code>objectClass: olcModuleList</code></div><div><code>cn: module</code></div><div><code>olcModulepath: /usr/lib/ldap</code></div><div><code>olcModuleload: back_hdb</code></div><div><code><br /></code></div><div><code># Database settings</code></div><div><code>dn: olcDatabase=hdb,cn=config</code></div><div><code>objectClass: olcDatabaseConfig</code></div><div><code>objectClass: olcHdbConfig</code></div><div><code>olcDatabase: {1}hdb</code></div><div><code>olcSuffix: dc=swearing,dc=com</code></div><div><code>olcDbDirectory: /var/lib/ldap</code></div><div><code>olcRootDN: cn=admin,dc=swearing,dc=com</code></div><div><code>olcRootPW: Sw3ar1ngatC0mputerz</code></div><div><code>olcDbConfig: set_cachesize 0 2097152 0</code></div><div><code>olcDbConfig: set_lk_max_objects 1500</code></div><div><code>olcDbConfig: set_lk_max_locks 1500</code></div><div><code>olcDbConfig: set_lk_max_lockers 1500</code></div><div><code>olcDbIndex: objectClass eq</code></div><div><code>olcLastMod: TRUE</code></div><div><code>olcDbCheckpoint: 512 30</code></div><div><code>olcAccess: to attrs=userPassword by dn="cn=admin,dc=swearing,dc=com" write by anonymous auth by self write by * none</code></div><div><code>olcAccess: to attrs=shadowLastChange by self write by * read</code></div><div><code>olcAccess: to dn.base="" by * read</code></div><div><code>olcAccess: to * by dn="cn=admin,dc=swearing,dc=com" write by * read</code></div><div></div></blockquote><br /></div><div>Go through this file before loading it. &nbsp;Change the &#8220;swearing&#8221;s to match your domain, and change that &#8220;olcRootPW&#8221; to a root password that you&#8217;ll remember. &nbsp;Don&#8217;t worry, it will be stored encrypted. &nbsp;This is just to set it up. &nbsp;Really all we&#8217;re doing setting up slapd to read it&#8217;s configuration from LDAP, and giving it login information for an admin account that will have access to the config.</div><div><br /></div><div>Just paste this into a file, name it backend.ldif , and then run:<br /><div><br /><blockquote><code>sudo ldapadd -Y EXTERNAL -H ldapi:/// -f backend.ldif&nbsp; </code></blockquote></div></div><div><b>4) Store stuff</b></div><div><b><br /></b></div><div>Technically, that&#8217;s it! &nbsp;Break out the champagne, you have LDAP! &nbsp;</div><div></div><div>In the real world though, you&#8217;ll want to set up your directory structure, and secure the whole thing. &nbsp;The easiest way to do mass changes is still by loading .ldif files, so go ahead and dump this baby into structure.ldif.</div><div></div><div><div><blockquote><code><br /># Create your root -level object - this should mirror your domain name</code></blockquote></div><div><blockquote><code><code><code>dn: dc=swearing,dc=com</code></code></code><code><code><code>objectClass: top</code></code></code><code><code><code>objectClass: dcObject</code></code></code><code><code><code>objectclass: organization</code></code></code><code><code><code>o: Swearing at Computers</code></code></code><code><code><code>dc: swearing</code></code></code><code><code><code>description: Swearing at Computers sample structure&nbsp;</code></code></code><code><code><code><br /></code></code></code><code><code><code># Create your Admin user - this should match what you told the back-end!</code></code></code><code><code><code>dn: cn=admin,dc=swearing,dc=com</code></code></code><code><code><code>objectClass: simpleSecurityObject</code></code></code><code><code><code>objectClass: organizationalRole</code></code></code><code><code><code>cn: admin</code></code></code><code><code><code>description: Swearing at Computers admin account</code></code></code><code><code><code>userPassword:&nbsp;Sw3ar1ngatC0mputerz</code></code></code><code><code><code><br /></code></code></code><code><code><code># Create an OU to store a certain kind of contact in</code></code></code><code><code><code>dn: ou=humans,dc=swearing,dc=com</code></code></code><code><code><code>objectClass: organizationalUnit</code></code></code><code><code><code>ou: humans</code></code></code><code><code><code><br /></code></code></code><code><code><code># Create an OU to store another kind of contact in. &nbsp;OK, you probably don't need this one</code></code></code><code><code><code>dn: ou=cats,dc=swearing,dc=com</code></code></code><code><code><code>objectClass: organizationalUnit</code></code></code><code><code><code>ou: cats</code></code></code><code><code><code><br /></code></code></code><code><code><code># Create a couple of groups</code></code></code><code><code><code>dn: ou=friends,dc=humans,dc=swearing,dc=com</code></code></code><code><code><code>objectClass: organizationalUnit</code></code></code><code><code><code>ou: friends</code></code></code><code><code><code><br /></code></code></code><code><code><code>dn: ou=enemies,dc=humans,dc=swearing,dc=com</code></code></code><code><code><code>objectClass: organizationalUnit</code></code></code><code><code><code>ou: enemies</code></code></code><code><code><code><br /></code></code></code><code><code><code># Create a POSIX group - you know, the kind that handle permissions on your server</code></code></code><code><code><code><br /></code></code></code><code><code><code>dn: cn=administrators,ou=humans,dc=swearing,dc=com</code></code></code><code><code><code>objectClass: posixGroup</code></code></code><code><code><code>cn: administrators</code></code></code><code><code><code>gidNumber: 90000</code></code></code><code><code><code><br /></code></code></code><code><code><code><br /></code></code></code><code><code><code># Create a person, who has a login account in the administrators group</code></code></code><code><code><code><br /></code></code></code><code><code><code>dn: uid=Peter,ou=friends,dc=swearing,dc=com</code></code></code><code><code><code>objectClass: inetOrgPerson</code></code></code><code><code><code>objectClass: posixAccount</code></code></code><code><code><code>objectClass: shadowAccount</code></code></code><code><code><code>uid: pgriffin</code></code></code><code><code><code>sn: Griffin</code></code></code><code><code><code>givenName: Peter</code></code></code><code><code><code>cn: Peter Griffin</code></code></code><code><code><code>displayName: Peter Griffin</code></code></code><code><code><code>uidNumber: 9000</code></code></code><code><code><code>gidNumber: 90000</code></code></code><code><code><code>userPassword: 12345?anidiotssuitcase</code></code></code><code><code><code>gecos: Peter Griffin</code></code></code><code><code><code>loginShell: /bin/bash</code></code></code><code><code><code>homeDirectory: /home/pgriffin</code></code></code><code><code><code>shadowExpire: -1</code></code></code><code><code><code>shadowFlag: 0</code></code></code><code><code><code>shadowWarning: 7</code></code></code><code><code><code>shadowMin: 8</code></code></code><code><code><code>shadowMax: 999999</code></code></code><code><code><code>shadowLastChange: 10877</code></code></code><code><code><code>mail: pgriffin@example.com</code></code></code><code><code><code>postalCode: 31000</code></code></code><code><code><code>l: Quahog</code></code></code><code><code><code>o: Toy Company</code></code></code><code><code><code>mobile: &nbsp;+1 (555) 555-1212</code></code></code><code><code><code>homePhone:&nbsp;+1 (555) 555-1212</code></code></code><code><code><code>title: Factory worker</code></code></code><code><code><code>postalAddress:&nbsp;</code></code></code><code><code><code>initials: PG</code></code></code></blockquote></div><span class="Apple-style-span" style="font-family: monospace;"><br /></span></div><br />Obviously, change those values to suit what you&#8217;re trying to do - but we just created a few organizational groups, and a POSIX group, and a user to go with them.  If you read through this carefully, you can get a sense of what all of these values do.  Basically, when you create that person you can define what kind of object it is.  Are they just an inetorgperson, with an email addy and contact info?  Are they a POSIX user?  Fields are created for you depending on what objectClasses you choose.  uidNumber and gidNumber are useless for someone who isn&#8217;t a POSIX user.<br /><br /><div><br /><b>5) Have a beer</b> <br /><b><br /></b><br />Seriously, you earned it.  That&#8217;s a lot of learning right there.  And at this very moment you have a working LDAP server!  This is fantastic.  Time to ask your boss for a raise!  Look at all the productivity you&#8217;ve got!  <br /><br />But wait, there&#8217;s more!  Right now, everything you&#8217;ve got is being sent in the clear.  Let&#8217;s fix that.<br /><br /><b>6) Set up your certificates</b><br /><br />There are two ways to encrypt data transmission in LDAP.  You can tunnel the whole connection through SSL, which is called LDAPS.  It works much the same way that HTTPS does compared to HTTP.  That&#8217;s the old way to do it, and technically it&#8217;s discouraged, but it&#8217;s perfectly good if your application supports it.  The other, recommended way, is TLS.  Basically TLS is encrypting the data transmitted within the LDAP stream.  <br /><br />Either way, it uses the same old SSL certificates that you use for HTTPS.  So if you&#8217;ve got those set up already, skip this step.  If not, never fear!  99% of secure LDAP deployments don&#8217;t need to worry about authenticating the server with an expensive external Certificate Authority.  All you should care about is the encryption, and that you can do with self signed certs.  So that&#8217;s what I&#8217;m covering here.<br /><br />6a) SLAPD is compiled with gnutls, which means you get the best behavior if you make your certs with the same library. <br /><div><blockquote><code><code><code>apt-get install gnutls-bin</code></code></code></blockquote></div>6b) The first step is to set up your faux Certificate Authority.  So make a private key for this CA:<br /><blockquote><code> sudo sh -c "certtool --generate-privkey &gt; /etc/ssl/private/faux-ca.key"</code></blockquote>6c) Now make a details file for your faux CA. Create /etc/ssl/faux-ca.info with the following content:<br /><blockquote><code><code><code> cn = Faux Certificate Authority<br /></code></code></code><br /><div><code><code><code>ca </code></code></code><br /><code><code><code>cert_signing_key</code></code></code></div></blockquote><div>6d) Make the self-signed CA certificate<br /><blockquote><code><code><code>sudo certtool --generate-self-signed --load-privkey /etc/ssl/private/faux-ca.key --template /etc/ssl/faux-ca.info --outfile /etc/ssl/certs/faux-ca.cert</code></code></code></blockquote></div>Now you&#8217;re a certificate authority!  Time to make your server&#8217;s key and sign it.<br /><br />6e) Make the server&#8217;s private key:<br /><blockquote><code><code><code>sudo sh -c "certtool --generate-privkey &gt; /etc/ssl/private/ldap.key</code></code></code></blockquote>6f) Make an .info file so your CA can sign the private cert.  I put mine at /etc/ssl/ldap.info :<br /><blockquote><code><code><code> organization = Swearing at Computers<br /></code></code></code><br /><div><code><code><code>cn = ldap.swearing.com&nbsp;</code></code></code></div><div><code><code><code>tls_www_server&nbsp;</code></code></code></div><div><code><code><code>encryption_key&nbsp;</code></code></code></div><div><code><code><code>signing_key</code></code></code></div></blockquote><div>6g) Sign the server&#8217;s certificate with your faux CA:<br /><blockquote><code><code><code>sudo certtool --generate-certificate --load-privkey /etc/ssl/private/ldap.key --load-ca-certificate /etc/ssl/certs/faux-ca.cert --load-ca-privkey /etc/ssl/private/faux-ca.key --template /etc/ssl/ldap.info --outfile /etc/ssl/certs/ldap.cert</code></code></code></blockquote></div><b>7) Add the key to SLAPD</b><br />Edit /etc/ldap/slapd , and uncomment the line that starts with SLAPD_SERVICES.  It should look like this when you&#8217;re done.<br /><blockquote><code>SLAPD_SERVICES="ldap:/// ldapi:/// ldaps:///"</code></blockquote>Now we are going to use that LDAP configuration interface you set up to add the keys:<br /><div><div><blockquote><code>sudo ldapmodify -Y EXTERNAL -H ldapi:///</code></blockquote></div>It will ask you for your LDAP password, which you set in the .ldif files way back in the day.  Then you just get an empty prompt.  Type these lines so see the magic happen:<blockquote><div><code><code><code>dn: cn=config</code></code></code></div><div><code><code><code>add: olcTLSCACertificateFile</code></code></code></div><div><code><code><code>olcTLSCACertificateFile: /etc/ssl/certs/faux-ca.cert</code></code></code></div><div><code><code><code>-</code></code></code></div><div><code><code><code>add: olcTLSCertificateFile</code></code></code></div><div><code><code><code>olcTLSCertificateFile: /etc/ssl/certs/ldap.cert</code></code></code></div><div><code><code><code>-</code></code></code></div><div><code><code><code>add: olcTLSCertificateKeyFile</code></code></code></div><div><code><code><code>olcTLSCertificateKeyFile: /etc/ssl/private/ldap.key</code></code></code></div></blockquote>Press enter a couple of times, and it will tell you that it&#8217;s modifying cn=config.  Press Ctrl+D to exit ldapmodify.<br /><br />Make sure that SLAPD&#8217;s user (openldap on ubuntu) has read access to the certificate files you named there.  For self-signed people, you can just set openldap as the owner of the files.  But if you share certs with another application (like http), you have to get tricky.  I just add both www-data and openldap to a new group called &#8220;ssl&#8221;, and give the new group read access.<br /><br />Sadly, you have to restart SLAPD to make it read that /etc/ldap/slapd change you made.  But after that, you&#8217;re up and running!  That&#8217;s it!<br /><br />Guaranteed you&#8217;re going to run into trouble at some stage here.  You&#8217;re going to get a cryptic error message that you won&#8217;t be able to interpret.  Don&#8217;t panic.  The problem is probably that SLAPD doesn&#8217;t have the right access to the certificate files.   Seriously.  I swore at the computer for 2 days trying to figure that one out.<br /><br />Have fun!</div></div>
</div>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/05/updating-kernels-for-amazon-aws.html">Updating Kernels for Amazon (AWS) Instances</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2011-05-09T17:27:00+02:00" pubdate data-updated="true">May 9<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content">
<div class="css-full-post-content js-full-post-content">
Lately I&#8217;ve been getting hit by an odd kernel bug on one of the servers I maintain. &nbsp;It&#8217;s discussed in some depth on <a href="https://bugs.launchpad.net/ubuntu/+source/linux-ec2/+bug/614853">Launchpad</a>, but the gist is that when trying to find an idle cpu for a new thread, the scheduler divides by cpu_power. &nbsp;On EC2 instances, it&#8217;s possible for cpu_power to be 0, which causes a divide by zero kernel crash. &nbsp;<u>Why</u> cpu_power is 0 is still an unknown; this should never be the case. &nbsp;But in the meantime, there&#8217;s a patch committed to ec2 kernel 2.6.32-313.26 which adds a check before running this division. &nbsp;If cpu_power is 0, it waits a bit and tries again. &nbsp;I believe it also adds a kernel yell, to help with tracking down the underlying problem. &nbsp;In the meantime, this bug was crashing one of my instances.<br /><br />So it&#8217;s time to update the kernel. &nbsp;This should be easy, right? &nbsp;apt-get dist-upgrade seems to do it&#8230; but if you reboot, uname -r will tell you you&#8217;re still on the old kernel. &nbsp;That&#8217;s when the swearing begins.<br /><br />You see, cloud based hosting is a very seductive thing. &nbsp;Those servers LOOK like real servers. &nbsp;They FEEL like real servers. &nbsp;And in 99% of all ways they ACT like real servers. &nbsp;But they are not real servers. &nbsp;And that 1% of difference is where all of the pain comes in. &nbsp;As an aside, succumbing to this illusion is setting yourself up for disaster with any cloud based server.<br /><br />One of the differences is that you don&#8217;t have a real boot sector, or a real BIOS. &nbsp;Everything you know about the first steps of booting a system has to go out the window. &nbsp;Amazon uses Xen virtualization, which means that all that boot stuff you know should be replaced with <a href="http://wiki.xensource.com/xenwiki/PvGrub">PVGrub</a>. &nbsp;Actually, don&#8217;t bother learning too much about it; we&#8217;re just interested in upgrading our kernel.<br /><br />Amazon <a href="http://ec2-downloads.s3.amazonaws.com/user_specified_kernels.pdf">documents this in a PDF</a>, which is a bit painful to work through. &nbsp;I&#8217;ve walked you through my process here.<br /><br />The first thing to know is that you need kernel support for PVGrub (or PV-Grub, because they&#8217;re inconsistent with the naming). &nbsp;Don&#8217;t sweat this too much, pvgrub is supported in most big distros. &nbsp;Amazon claims to have tested with these ones:<br /><br />• Fedora 8‐9 Xen kernels<br />• Fedora 13 (2.6.33.6‐147 and higher)<br />• SLES/openSUSE 10.x, 11.0, 11.1 Xen<br />• SLES/openSUSE 11.x EC2 Variant<br />• Ubuntu EC2 Variant  kernels<br />• RHEL 5.x kernels <br />• RHEL 6.x kernels<br />• CentOS 5.x kernels<br />• Gentoo<br /><br />Ubuntu, at least, is smart enough to install the EC2 variant kernels in that apt-get dist-upgrade you ran earlier, so that much is done for you already. <br /><br />The next thing you need to know is that PVGrub has a strong historical connection to PXE booters. &nbsp;If you haven&#8217;t worked with network booting, that&#8217;s what PXE does for you. &nbsp;The main difference is that you have to go back to having separate initrd and vmlinuz images in your /boot directory. &nbsp;To be honest, I never trusted booting without vmlinuz, so this made me a little happy. &nbsp;It made me even happier to see that apt-get had dropped both images in there for me already. <br /><br />Now you have to create a grub boot list. &nbsp;On a real server, this is the file which tells Grub which kernels are options for booting. &nbsp;Create /boot/grub/menu.lst with the following content:<br /><br /><blockquote><code> default 0</code><code> timeout 3<br /></code><code> title EC2<br /></code><code> root (hd0)<br /></code><code> kernel /boot/vmlinuz-2.6.32-314-ec2 root=/dev/sda1</code><code> initrd /boot/initrd.img-2.6.32-314-ec2</code></blockquote><br />Those are the vmlinuz and initrd filenames that I got from my update - since you&#8217;re following this guide somewhere in the future, those version numbers will be different. &nbsp;Have a quick look to see what you&#8217;ve got in /boot , and replace those filenames in your own menu.lst.<br /><br />That&#8217;s it - but in order to make this active, you have to bundle it as an AMI and start a new instance. &nbsp;This actually enforces a very good, conservative workflow. &nbsp;You get to run your updates, and test the kernel update on a non-live environment before moving your live IP over. <br /><br />Because you&#8217;re using a non-standard kernel, you do have to manually specify a few things in your AMI build process. &nbsp;Here&#8217;s the run-down:<br /><blockquote><code> # ec2-bundle-vol -r [ARCH] -d [DESTINATION] -p [AMI-NAME] -u [AWS-USERID] -k [KEYFILE] -c [CERTFILE] -s 10240 -e [EXCEPTIONS] --kernel [KERNEL-ID]</code></blockquote><div>This is your standard <a href="http://docs.amazonwebservices.com/AmazonEC2/dg/2006-10-01/CLTRG-ami-bundle-vol.html">ec2-bundle-vol</a> command. &nbsp;If you don&#8217;t use this too often, here are the variables:<br /><br /><ul><li>[ARCH] is the CPU architecture for your instance. &nbsp;It&#8217;s either i386 or x86_64</li><li>[DESTINATION] is the temporary filespace to use when building the AMI. &nbsp;<b>Do not do this on the root partition</b>. &nbsp;I use /mnt , which is the 100gb EBS volume provided with the instance.</li><li>[AMI-NAME] is the name for the AMI. It can be anything, I used production-kernel-2.6.32-314-ec2</li><li>[AWS-USERID] Your AWS userID number. &nbsp;You can find it in the top right corner of the AWS account page.</li><li>[KEYFILE] and [CERTFILE] are your AWS keyfile and certificate files, which are used to authenticate with AWS.</li><li>[EXCEPTIONS] is a comma delineated list of directories to leave out of the image file. &nbsp;In my case this is /mnt,/tmp,/ebs . &nbsp;Note that if you leave /tmp out, the tempdir created automatically will have incorrect permissions.</li></ul><div>[KERNEL-ID] doesn&#8217;t fit into a bullet point. &nbsp;This is an ID that identifies the architecture of the instance for Amazon&#8217;s bootloader, and it&#8217;s important. &nbsp;Choose the right KID depending on your CPU, instance type, and region:</div><div><div><table><tbody><tr><td></td><td><b>32-bit Instance</b></td><td><b>64-bit Instance</b></td><td><b>32-bit EBS</b></td><td><b>64-bit EBS</b></td></tr><tr><td><b>US-East-1</b></td><td>aki-407d9529</td><td>aki-427d952b</td><td>aki-4c7d9525</td><td>aki-4e7d9527</td></tr><tr><td><b>US-West-1</b></td><td>aki-99a0f1dc</td><td>aki-9ba0f1de</td><td>aki-9da0f1d8</td><td>aki-9fa0f1da</td></tr><tr><td><b>EU-West-1</b></td><td>aki-4deec439</td><td>aki-4feec43b</td><td>aki-47eec433</td><td>aki-41eec435</td></tr><tr><td><b>AP-SouthEast-1</b></td><td>aki-13d5aa41</td><td>aki-11d5aa43</td><td>aki-6fd5aa3d</td><td>aki-6dd5aa3f</td></tr></tbody></table></div><div><br /></div><div>One important note: in Amazon&#8217;s PDF documentation, they use an odd character set. &nbsp;This means that the hyphen in the middle of the AKI is not a unicode hyphen. &nbsp;I spent more than an hour trying to figure out why my AMI was complaining of an invalid AKI over this! &nbsp;Either copy and paste the ID from this blog post, or type it in by hand. <br /><br />From here on in ,it&#8217;s a standard AMI upload and register process:<br /><br /><blockquote><code> # ec2-upload-bundle -b [BUCKET-LOCATION] -m [MANIFEST] -a [KEY] -s [SECRET KEY]</code></blockquote><br /><br /><ul><li>[BUCKET-LOCATION] is the name of an S3 bucket where you want to save this AMI. &nbsp;If the bucket doesn&#8217;t exist, ec2-upload-bundle will create it.</li><li>[MANIFEST] is the manifest file created with ec2-bundle-vol above. &nbsp;It&#8217;ll be in the same directory, with the same name, just with the extension .manifest.xml . &nbsp;For example, /mnt/production-kernel-2.6.32-314-ec2.manifest.xml</li><li>[KEY] and [SECRET KEY] are your AWS key and secret key, which you can get from your AWS Account area.</li></ul><br /><code><br /># ec2-register –-name [AMI NAME] [S3 MANIFEST]<br /></code><br /><br /><br /><ul><li>[AMI NAME] is the name of your AMI (duh)</li><li>[S3 MANIFEST] is the path of the manifest file you just uploaded, including the bucket path, on S3. &nbsp;For example prod-upgrade/production-kernel-2.6.32-314-ec2.manifest.xml</li></ul>Now start your new AMI, and test like hell before replacing that production instance!&nbsp;</div></div></div>
</div>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/04/simple-aws-backups-with-snapshots-and.html">Simple AWS Backups With Snapshots and AMIs</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2011-04-30T15:26:00+02:00" pubdate data-updated="true">Apr 30<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content">
<div class="css-full-post-content js-full-post-content">
Are you just getting started with Amazon Web Services? &nbsp;It&#8217;s a great service, but it faces some reliability challenges. &nbsp;This post explains how to set up a simple backup system using Amazon&#8217;s &#8220;snapshot&#8221; &#8220;machine image&#8221; functionalities.<br /><br /><b>Dependencies</b><br /><br />Before you begin, you will need:<br /><br /><ul><li>Amazon&#8217;s standard <a href="http://aws.amazon.com/developertools/351?_encoding=UTF8&amp;jiveRedirect=1">EC2 Command Line Tools</a></li><li>Alestic&#8217;s <a href="http://alestic.com/2009/09/ec2-consistent-snapshot">ebs-consistent-snapshot</a> program</li><li>Tech Kismet&#8217;s <a href="http://remove-old-snapshots.php/">remove-old-snapshots.php</a> script (which must be so named)</li></ul><br /><div><br /></div><div><b>AMI Images</b></div><div><b><br /></b></div><div>Everyone knows that Amazon Instances are ephemeral. &nbsp;When you shut down your server, it&#8217;s really gone. So the first thing you want to do is create an &#8220;Amazon Machine Image&#8221; of your server. &nbsp;You can use an AMI to start new instances anytime you like, and the backup is stored on S3 so it is region-independent. &nbsp;Anyone who felt the EBS regional downtime last week is looking for region-independent backup solutions.</div><div><br /></div><div>So here&#8217;s a little script I wrote to do a regular backup of your server to AMI:</div><br /><blockquote><code><br />#!/bin/bash<br /># Variables.  Change this to suit the server.  Exceptions should be a comma-separated list of directories to exclude from the backup.&nbsp;</code>&nbsp;</blockquote><blockquote><code> SERVER="server-name" #the name of the server you are backing up&nbsp;</code><br /><div><code>UID="1234-1234-1234" # Your Amazon user ID number&nbsp;</code></div><div><code>CERT="/path/to/your/certfile.pem" # Your Amazon certificate file&nbsp;</code></div><div><code>KEY="/path/to/your/key.pem" #Your Amazon key file</code></div><div><code>ACCESSKEY="12345567823SSDFGS6" #your Amazon access key</code></div><div><code>SECRETKEY="aSDFOUxsfoiusfSDF12SF/Sfi0Asgoiwre" #your amazon secret key&nbsp;</code><br /><div><code>EXCEPTIONS="/ebs,/mnt" #Directories you want to exclude&nbsp;</code><br /><div><code>DESTINATION="/mnt" #Temporary filespace for building the backup itself&nbsp;</code></div><br /><div><code>ARCH="x86_64" # Is your Instance 32 or 64 bit? <br />DATE=`date +%Y-%m-%d` #today's date<br /><br /><br /># set up nice error handling function error_exit { echo "$1" 1&gt;&amp;2 exit 1 }<br />rm -rf "$DESTINATION"/backup* echo "$(date) Starting instance backup script." echo "creating backup bundle..."<br />if /usr/bin/ec2-bundle-vol -u $UID -c $CERT -k $KEY -p backup-$SERVER-$DATE -r $ARCH -e "$EXCEPTIONS" -d "$DESTINATION"; then   echo "SUCCESS" else    error_exit "ERROR creating bundle. Exiting..." fi echo "uploading backup bundle..."<br />if /usr/bin/ec2-upload-bundle -b backup  -m "$DESTINATION"/backup-$SERVER-$DATE.manifest.xml -a $ACCESSKEY -s $SECRETKEY; then    echo "SUCCESS" else   error_exit "ERROR uploading bundle.  Exiting..." fi<br />echo "registering backup bundle with AWS..."<br />if ec2-register -K $KEY -C $CERT backup/backup-$SERVER-$DATE.manifest.xml; then   echo "SUCCESS" else   error_exit "ERROR registering bundle.  Exiting..." fi<br />echo "deleting backup cache..."<br />if rm -rf "$DESTINATION"/backup*; then   echo "SUCCESS" else   error_exit "ERROR deleting backup cache.  Exiting..." fi<br />echo "backup commands run successfully"<br />exit 0<br /></code></div></div></div></blockquote><br />Run this guy on cron, and you have a regular backup going. &nbsp;The one thing it does not do is prune old backups. &nbsp;Most server setups are fire-and-forget - this is a way of documenting that firing process for next time.<br /><br /><b>EBS Snapshots</b><br /><b><br /></b><br />The counterpart to Instances are EBS data stores. &nbsp;These are block devices that provide permanent storage on Amazon&#8217;s cloud. &nbsp;Here&#8217;s my backup script for an attached EBS store.<br /><br /><code><br /></code><br /><blockquote><code> #!/bin/bash</code><code><br /></code><code> # Variables. &nbsp;Change this to suit the server. &nbsp;Mysql User needs access to lock and unlock tables. Number is how many old backup snapshots to keep.</code><code><br /></code><code> SERVER="server-name" #the name of the server you are backing up, for labeling</code><code> VOLUME="vol-1234567a" #the volume ID that you are backing up</code><code> MOUNTPOINT="/ebs" #where the volume is mounted</code><code> MYSQLUSER="root" #a mysql user with access to lock all tables</code><code> MYSQLPASS="password" #password for the mysql user</code><code> KEYID=12345799asf8675asf #your Amazon key</code><code> SECRETKEY=asf765sdg876wrkhalsfa/234@$ &nbsp;# your Amazon secret key</code><code> NUMBER="15" # How many old backups to keep</code><code> DATE=`date +%Y-%m-%d` #today's date</code><code><br /></code><code><br /></code><code> # set up nice error handling</code><code> function error_exit</code><code> {</code><code> &nbsp; &nbsp; &nbsp; &nbsp; echo "$1" 1&gt;&amp;2</code><code> &nbsp; &nbsp; &nbsp; &nbsp; exit 1</code><code> }</code><code><br /></code><code> echo "$(date) Starting EBS backup script."</code><code> echo "creating EBS snapshot..."</code><code><br /></code><code> if /usr/bin/ec2-consistent-snapshot --description "$SERVER backup $DATE" --aws-access-key-id $KEYID --aws-secret-access-key $SECRETKEY --mysql-username $MYSQLUSER --mysql-password $MYSQLPASS --xfs-filesystem "$MOUNTPOINT" $VOLUME;&nbsp;</code><span class="Apple-style-span" style="font-family: monospace;">then</span><code> &nbsp; echo "SUCCESS"</code><code> else</code><code> &nbsp; error_exit "ERROR creating snapshot. Exiting..."</code><code> &nbsp; exit 1</code><code> fi</code><code><br /></code><code> if /usr/bin/php /usr/sbin/remove-old-snapshots.php -v $VOLUME -n $NUMBER ; then</code><code> &nbsp; echo "SUCCESS"</code><code> else</code><code> &nbsp; error_exit "ERROR removing old snapshots. &nbsp;Exiting..."</code><code> &nbsp; exit 1</code><code> fi</code><code><br /></code><code> echo "backup commands run successfully"</code><code> exit 0</code></blockquote><br /><code></code><br /><div><br />This little guy is very useful if you have an EBS device that you use for a running MySQL instance. It will lock MySQL tables briefly, and start the snapshot process from Amazon. &nbsp;This one does prune old backups, you can specify how many you want to keep. <br /><br />The biggest down side to the EBS snapshot is that it is zone-locked. &nbsp;An EBS snapshot can only be used in the zone where it was created. &nbsp;So in the case of a zone-specific outage, you&#8217;re SOL.<br /><br />This is not a proper, region-independent, auto-pruning, tested backup system. &nbsp;This is a set of basic element scripts that you can run on cron, and have some peace of mind while you look into something more robust. &nbsp; &nbsp;But it is a good start, at least!</div>
</div>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/04/aws-load-balancer-ssl-limitations.html">AWS Load Balancer SSL Limitations</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2011-04-29T13:58:00+02:00" pubdate data-updated="true">Apr 29<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content">
<div class="css-full-post-content js-full-post-content">
No one else seems to have documented this anywhere, so I&#8217;m gonna do it here.<br /><br />Amazon Web Services has a great little load balancer system you can use. &nbsp;A few clicks, and you&#8217;re away to the races with a shiny load balancer of your own! &nbsp;The best part? &nbsp;It will even do SSL termination at the load balancer for you. &nbsp;Just paste in your certs and away you go! &nbsp;Right?<br /><br />Wrong. &nbsp;Seems like every time I do this, I end up with an invalid certificate at some stage of the game. Amazon doesn&#8217;t tell you WHAT about your cert is wrong, or even what cert formats they want. &nbsp;They just say &#8220;error: invalid private key&#8221;.<br /><br />I use a lot of Comodo certificates, which take about a day to generate. &nbsp;I don&#8217;t know why it takes them that long, maybe they&#8217;re lovingly hand crafted by artisanal SSL certificate islanders on a small pacific island. &nbsp;The point is, it takes for ever for them to respond to a request for a new cert, and that means I don&#8217;t like to sit around regenerating certs in different formats at random until I figure out what Amazon wants.<br /><br />So here&#8217;s what I worked out - you can do this at home yourself. &nbsp;I generated my private key and certificate request with the often used:<br /><blockquote>openssl req -new -nodes -keyout swearingatcomputers.key -out swearingatcomputers.csr</blockquote>This is the lazy man&#8217;s approach. &nbsp;It doesn&#8217;t bother me about a password for the key, I don&#8217;t have to type two separate commands&#8230; I just get a nice quick key that I can use. &nbsp;Comodo accepts the key and certificate request, and 24 hours later my signed public certificate and authority chain file are delivered in the mail. &nbsp;When I set up SSL with Apache, this is fine.<br /><br />But try and drop this into Amazon, and you get one of those mysterious messages &#8220;error: invalid private key&#8221;. &nbsp;Turns out your keys have to be RSA or DSA encrypted in order for Amazon to accept them. &nbsp;To see if you&#8217;re affected, just look at the first line of your key file. &nbsp;If it says &#8220;BEGIN PRIVATE KEY&#8221;, then read on. &nbsp;If it says &#8220;BEGIN RSA PRIVATE KEY&#8221; or &#8220;BEGIN DSA PRIVATE KEY&#8221;, then this won&#8217;t interest you , sorry.<br /><br />So to fix this problem, you SHOULD have used a slightly different command to generate that key:<br /><br /><blockquote>openssl req -nodes -newkey rsa:2048 -keyout swearingatcomputers.key -out swearingatcomputers.csr</blockquote>At this point, I started swearing at computers. &nbsp;I have to wait another 24 hours for some pacific islander to meticulously hand-paint <b>another</b>&nbsp;cert? &nbsp;Ridiculous!<div><br /></div><div>But don&#8217;t you fret. &nbsp;You can actually convert the certs you have into RSA versions that Amazon will love. First, the private key:</div><blockquote>openssl rsa -in swearingatcomputers.key -text</blockquote>This will spit out all the calculations openssl has to do to read the key, and at the end - an RSA encrypted key! &nbsp;Just copy and paste the RSA PRIVATE KEY section at the end (including the BEGIN and END lines) into a separate file, or into AWS directly, and there ya go! &nbsp;<br /><br />In order to make the public certificate match, you&#8217;ll have to convert that, too.<br /><br /><blockquote>openssl x509 -inform PEM -in swearingatcomputers.crt</blockquote><div>BAM - out comes your fancy key for Amazon usage.</div><div><br /></div><div>And that&#8217;s it! &nbsp;I&#8217;m happy SOMEONE took the time to document Amazon&#8217;s SSL key requirements. &nbsp;They make sense, they&#8217;re smart requirements&#8230; but they have to be written somewhere for poor rubes like me.</div>
</div>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/2011/04/multiple-ssl-sites-on-one-aws-instance.html">Multiple SSL Sites on One AWS Instance</a></h1>
    
    
      <p class="meta">
        








  



  
<time datetime="2011-04-29T13:44:00+02:00" pubdate data-updated="true">Apr 29<span>th</span>, 2011</time>
        
      </p>
    
  </header>


  <div class="entry-content">
<div class="css-full-post-content js-full-post-content">
It&#8217;s a familiar problem - you can&#8217;t really host multiple SSL sites on a single IP address. &nbsp;There&#8217;s a fancy workaround if you get a pricey multi-domain certificate, of course. &nbsp;But separate sites, with separate certificates just won&#8217;t work. &nbsp;The reason is simple - put yourself in Apache&#8217;s shoes.<br /><br />When you&#8217;re using name based virtualhosts, Apache uses the request headers to determine which vhost should get the request. &nbsp;But HTTPS headers are encrypted! &nbsp;So there&#8217;s no way for Apache to tell which virtual host should get this message, without decrypting it furst. &nbsp;But it can&#8217;t decrypt without knowing the correct virtual host&#8230;<br /><br />There&#8217;s a great workaround for this with Amazon, using their Elastic Load Balancer (ELB) system. &nbsp;You simply set up a load balancer, and forward port 443 to, say port 8443 on your instance, and have Apache listen on 8443 for SSL connections. &nbsp;Recently Amazon rolled out the ability to terminate SSL on the load balancer, so you can actually have the ELB listen on 443, decrypt the traffic with your certs, and forward the request to your Instance the clear, on port 80. <br /><br />There is one weakness. &nbsp;ELBs cannot be addressed by IP address. They can only be addressed by CNAME - and can anyone think of why this might cause problems? &nbsp;If you said &#8220;your root DNS record can&#8217;t be a CNAME&#8221;, go get yourself a glass of milk and some oreos, you&#8217;ve earned them. &nbsp;Now this is one of those DNS rules that is often ignored. &nbsp;For most people, having a CNAME for swearingatcomputers.com really isn&#8217;t going to break anything. &nbsp;But for anyone who uses email on their domain, this is an important rule to follow. &nbsp;Your MX records require that there be an A record for the domain. <br /><br />Still, this will get you to a pretty good place. &nbsp;You can have https://secure.swearingatcomputers.com , separate from http://www.swearingatcomputers.com , and that fits a lot of use cases.<br /><br />Not all of them, though. &nbsp;Sometimes you have a client who simply MUST have SSL for everything. &nbsp;Now you&#8217;re in trouble. &nbsp;Here are your options:<br /><br />1) Set up your own load balancer on a separate, micro instance. &nbsp;ELB is nice, but if it can&#8217;t do what you want, you gotta do it the old fashioned way.<br /><br />2) Set up a simple Apache instance with the certs installed, and &#8220;redirect permanent&#8221; to www.swearingatcomputers.com, which is your ELB CNAME.<br /><br />3) Cry about it.<br /><br />I tried option 3, but it didn&#8217;t help. &nbsp;Which of the other two options would you take?
</div>

</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/7/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/5/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>About Me</h1>
  <p>A little something about me.</p>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2017/02/11/a-year-between-the-cracks-of-decoupled-drupal/">Between the Cracks of Decoupled (Drupal) Architecture</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/11/05/writing-drupal-8-code-for-drupal-7/">Writing Drupal 8 Code for Drupal 7</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/10/31/developer-options-for-replacing-your-old-macbook-pro/">Developer Options for Replacing Your Old MacBook Pro</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/10/15/perfect-linux-mint-macbook-pro-9-2/">Perfect Linux Mint on My Macbook Pro 9,2</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/08/13/getting-my-notes-out-of-evernote/">Getting My Notes Out of Evernote</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/ohthehugemanatee">@ohthehugemanatee</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'ohthehugemanatee',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>




<section>
<a class="twitter-timeline"  href="https://twitter.com/campbellvertesi"  data-widget-id="407927984901746688">Tweets by @campbellvertesi</a>
    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>

    </section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - Campbell Vertesi (ohthehugemanatee) -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'ohthehugemanatee';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
